{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "WqKhCkps7Ph9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Lwphk3l6_f7"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "import pickle\n",
        "import blosc\n",
        "\n",
        "from scipy.io.wavfile import write\n",
        "#from tqdm.notebook import tqdm\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "from bark import SAMPLE_RATE, generate_audio, preload_models\n",
        "from IPython.display import Audio\n",
        "from multiprocessing import Process\n",
        "\n",
        "preload_models()\n",
        "\n",
        "print(\"Loaded Bark\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set Variables"
      ],
      "metadata": {
        "id": "pPfXpe857u_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 45   #Texts are split by sentence but in case no punctuation is detected or a sentence is too long, we split further by number of tokens\n",
        "\n",
        "SPEAKER = \"v2/en_speaker_6\" #Set bark speaker\n",
        "silence = np.zeros(int(0.25 * SAMPLE_RATE))  # quarter second of silence\n",
        "\n",
        "SAVE_PATH = \"_OUTPUT/\"\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)"
      ],
      "metadata": {
        "id": "4Hv1Vpzx7wXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Data"
      ],
      "metadata": {
        "id": "c7dITlof7ZCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #Delete\n",
        "# from DataLoader import DataLoader\n",
        "# import Utilities\n",
        "# from Utilities import *\n",
        "\n",
        "# datasetName = \"Twitter\"\n",
        "# data = DataLoader(\"Twitter\")\n",
        "# texts = data.texts"
      ],
      "metadata": {
        "id": "EXg5Tw1f7cFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = []"
      ],
      "metadata": {
        "id": "WqpgtPpl7aW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3QSsuKsDaz4"
      },
      "source": [
        "#Load Resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKfgCH5aDQuG"
      },
      "outputs": [],
      "source": [
        "#Load list of emoticons\n",
        "#Source: https://c.r74n.com/faces\n",
        "\n",
        "with open(\"TextEmoticonList.txt\", \"r\") as file:\n",
        "  emoticonList = file.read().split(\"\\n\")\n",
        "\n",
        "#Remove emoticons with spaces in-between\n",
        "emoticonList = [emoticon for emoticon in emoticonList if len(emoticon.split(\" \")) == 1]\n",
        "\n",
        "#Remove one character emoticons\n",
        "emoticonList = [emoticon for emoticon in emoticonList if len(emoticon) > 1]\n",
        "\n",
        "print(len(emoticonList))\n",
        "print(emoticonList[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Knh2n_bAZOrF"
      },
      "outputs": [],
      "source": [
        "#Load list of emojis\n",
        "#Source: https://www.airtable.com/universe/exphjm5ifnV0bX4Kb/emojis-database?explore=true\n",
        "\n",
        "emojiList = pd.read_csv(\"Emojis-Grid view.csv\")\n",
        "emojiList = emojiList[emojiList[\"Emoji\"] != \"C\"]\n",
        "emojiList = emojiList[\"Emoji\"].tolist()\n",
        "\n",
        "#Unicode versions\n",
        "emojiList_uni = [emoji.encode('unicode-escape').decode('ASCII') for emoji in emojiList]\n",
        "\n",
        "print(len(emojiList))\n",
        "print(emojiList[:10])\n",
        "print(emojiList_uni[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WVujY7b0MpD"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text"
      ],
      "metadata": {
        "id": "E00tYga8oNC1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbws3oNj7nSP"
      },
      "outputs": [],
      "source": [
        "#FLAGS\n",
        "DEIDENTIFY = True     #Replace urls, emails, and usernames\n",
        "EMOPRESERVE = True    #Identify emojis/emoticons on text and skip text cleaning on them\n",
        "TEXTCLEAN = False     #Minimal cleaning of separating certain conjunctions\n",
        "TOKEN_TYPE = \"wp\"     #wp: word piece (BERT Tokenizer); ws: word split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D4_Mb-y0MpE"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "tokenURL = \"_URL_\"\n",
        "tokenEmail = \"_EMAIL_\"\n",
        "tokenUsername = \"_USER_\"\n",
        "reserveTokens = [tokenURL, tokenEmail, tokenUsername]\n",
        "\n",
        "#CLEANING PROCESS\n",
        "#- Include emojis and emoticons\n",
        "#- Replace url, email, and usernames with tokens\n",
        "#- Remove non-major puncutations and separate them from words with whitespaces\n",
        "#- Lowercase\n",
        "def preprocess_str(string):\n",
        "\n",
        "  #Preclean\n",
        "  if DEIDENTIFY:\n",
        "    string = re.sub(r\"https?://[^\\s]+\", tokenURL, string)              #Links\n",
        "    string = re.sub(r\"[\\w.+-]+@[\\w-]+\\.[\\w.-]+\", tokenEmail, string)   #Email\n",
        "    string = re.sub(r\"@[a-zA-Z0-9_]{2,}\", tokenUsername, string)       #Usernames\n",
        "\n",
        "  #Emoticon/Emoji split\n",
        "  tokens = [string]\n",
        "  if EMOPRESERVE:\n",
        "    allEmo = emoticonList + emojiList + emojiList_uni + reserveTokens\n",
        "    for emoticon in allEmo:\n",
        "      regEx = \"(^|\\s)\" + re.escape(emoticon) + \"(\\s|$)\" if emoticon.isalpha() else re.escape(emoticon)\n",
        "      if emoticon in string:\n",
        "        splits = []\n",
        "        for split in tokens:\n",
        "          splits.append(re.split(r\"(\" + regEx + \")\", split))\n",
        "        tokens = [y.strip() for x in splits for y in x if y != \"\"]\n",
        "\n",
        "  for idx in range(len(tokens)):\n",
        "    if EMOPRESERVE and tokens[idx] in allEmo: #Skip emoticons, emojis\n",
        "      continue\n",
        "\n",
        "    if TEXTCLEAN:\n",
        "      tokens[idx] = re.sub(r\"[^A-Za-z0-9(),!?\\.\\'\\`]\", \" \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\'s\", \" \\'s\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\'ve\", \" \\'ve\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"n\\'t\", \" n\\'t\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\'re\", \" \\'re\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\'d\", \" \\'d\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\'ll\", \" \\'ll\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\",\", \" , \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"!\", \" ! \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\(\", \" ( \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\)\", \" ) \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\?\", \" ? \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\.\", \" . \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\s{2,}\", \" \", tokens[idx])\n",
        "\n",
        "    #Lower case and strip by default\n",
        "    tokens[idx] = tokens[idx].lower().strip()\n",
        "\n",
        "  return \" \".join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Audio"
      ],
      "metadata": {
        "id": "CJThTvpH7q3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sentences(text):\n",
        "  sentences = nltk.sent_tokenize(text)\n",
        "  splits = []\n",
        "  for sentence in sentences:\n",
        "\n",
        "    if len(sentence.split(\" \")) > MAX_TOKENS: #Split in token lengths\n",
        "      temp = sentence.split(\" \")\n",
        "      phrases = []\n",
        "      while len(temp) > MAX_TOKENS:\n",
        "        phrases.append(\" \".join(temp[:MAX_TOKENS]))\n",
        "        temp = temp[MAX_TOKENS:]\n",
        "      else:\n",
        "        phrases.append(\" \".join(temp))\n",
        "      splits.extend(phrases)\n",
        "    else:\n",
        "      splits.append(sentence)\n",
        "\n",
        "  return splits"
      ],
      "metadata": {
        "id": "QfAq45ym7ths"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_clean = [preprocess_str(text) for text in texts]\n",
        "\n",
        "audio_list = []\n",
        "for i, text in enumerate(tqdm(texts_clean[:3])):\n",
        "  sentences = split_sentences(text)\n",
        "  pieces = []\n",
        "  for sentence in sentences:\n",
        "    audio_array = generate_audio(sentence, history_prompt=SPEAKER, silent = True)\n",
        "    pieces += [audio_array, silence.copy()]\n",
        "\n",
        "  #Save file\n",
        "  write(SAVE_PATH + \"Sample_%d.wav\" % i, SAMPLE_RATE, np.concatenate(pieces))"
      ],
      "metadata": {
        "id": "0gLZmv_u8TaK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}