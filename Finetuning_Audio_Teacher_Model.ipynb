{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jrQ_HH8NOIOR",
        "MQoWFSJ3OfHq",
        "HrcHaEmPQW6i",
        "6tHuG5etQbJV"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "-Zo-bv-f_AUK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xRmIu30-1jC"
      },
      "outputs": [],
      "source": [
        "import pandas as np\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "\n",
        "# from tqdm.notebook import tqdm\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "id": "GwZUtwww_Ivw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set Variables"
      ],
      "metadata": {
        "id": "dOuxKGmU_Coa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "SAVE_PATH = \"_TEMP_MODELS/\" #Path where models will be saved\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)"
      ],
      "metadata": {
        "id": "FlIvQh3U_QRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Data"
      ],
      "metadata": {
        "id": "8un4JW6-_EJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NOTE: Load audio using librosa\n",
        "# librosa.load(wavPath, sr = 16000) #AST required sample rate\n",
        "original_train_audio = []\n",
        "original_val_audio = []\n",
        "original_test_audio = []\n",
        "\n",
        "original_train_labels = []\n",
        "original_val_labels = []\n",
        "original_test_labels = []\n",
        "\n",
        "train_size = len(original_train_labels)\n",
        "val_size = len(original_val_labels)\n",
        "test_size = len(original_test_labels)\n",
        "\n",
        "all_audio = original_train_audio + original_val_audio + original_test_audio\n",
        "all_labels = np.array(original_train_labels + original_val_labels + original_test_labels)\n",
        "\n",
        "#LABELS\n",
        "unique_labels=np.unique(original_train_labels)\n",
        "num_class = len(unique_labels)\n",
        "\n",
        "lEnc = LabelEncoder()\n",
        "lEnc.fit(unique_labels)\n",
        "\n",
        "print(unique_labels)\n",
        "print(lEnc.transform(unique_labels))\n",
        "\n",
        "all_targets = lEnc.transform(all_labels)"
      ],
      "metadata": {
        "id": "4AdSyOr__Sk8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models"
      ],
      "metadata": {
        "id": "jrQ_HH8NOIOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "from torchvision import models, transforms\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "N9kBBoqpOJyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
        "from transformers import AutoFeatureExtractor, ASTForAudioClassification, ASTFeatureExtractor\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "def get_model(checkpoint, num_class, args = None):\n",
        "\n",
        "  if checkpoint == \"MIT/ast-finetuned-audioset-10-10-0.4593\":\n",
        "    ast = AST(num_class, args = args)\n",
        "    return ast\n",
        "  else:\n",
        "    raise Exception(\"Unknown checkpoint\")"
      ],
      "metadata": {
        "id": "Mj6rW2UqOPvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AST"
      ],
      "metadata": {
        "id": "05MDdEcmOZIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Wraps AST into model class that returns dict\n",
        "class AST(torch.nn.Module):\n",
        "  def __init__(self, num_class, args = None):\n",
        "    super(AST, self).__init__()\n",
        "\n",
        "    #Load config\n",
        "    config = AutoConfig.from_pretrained(checkpoint, num_labels = num_class)\n",
        "\n",
        "    if args != None:\n",
        "      config = AutoConfig.from_pretrained(checkpoint, num_labels = num_class, **args)\n",
        "    else:\n",
        "      config = AutoConfig.from_pretrained(checkpoint, num_labels = num_class)\n",
        "\n",
        "    #Load pretrained model\n",
        "    self.ast = ASTForAudioClassification.from_pretrained(checkpoint, ignore_mismatched_sizes = True, config = config)\n",
        "\n",
        "    #Change last classification layer output to num classes\n",
        "    # self.ast.classifier.dense = nn.Linear(in_features = 768, out_features = num_class, bias = True)\n",
        "\n",
        "  def forward(self, input_values, labels = None):\n",
        "\n",
        "    x = self.ast(input_values = input_values, labels = labels)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ELAs7MtvOb9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "MQoWFSJ3OfHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, inputs, labels, ids):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "        self.ids = ids\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if type(self.inputs) == dict:\n",
        "          item = {key: val[idx] for key, val in self.inputs.items()}\n",
        "        else:\n",
        "          item = {\"input\": self.inputs[idx]}\n",
        "\n",
        "        item[\"ids\"] = self.ids[idx]\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "N5hzXZjuOhJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_norm_stats(max_ast_length, sampling_rate):\n",
        "  temp = ASTFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", max_length = max_ast_length, do_normalize = False)\n",
        "  temp_input = temp(all_audio, sampling_rate = sampling_rate, return_tensors = \"pt\")\n",
        "\n",
        "  return torch.mean(temp_input[\"input_values\"]), torch.std(temp_input[\"input_values\"])"
      ],
      "metadata": {
        "id": "0gtFEqDFJw4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gather all inputs for all models\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def build_inputs(audio, max_lengths = {}, return_max_len = False):\n",
        "  inputs = {}\n",
        "\n",
        "  MAX_AST = 512\n",
        "  mean, std = get_norm_stats(MAX_AST, SAMPLING_RATE)\n",
        "\n",
        "  #Extract features\n",
        "  feature_extractor = ASTFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", max_length = MAX_AST, do_normalize = True, mean = float(mean), std = float(std))\n",
        "  input = feature_extractor(audio, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\")\n",
        "  print(\"AST -> Check mean 0, std 0.5:\", torch.mean(input[\"input_values\"]), torch.std(input[\"input_values\"]))\n",
        "\n",
        "  for k, v in input.items():\n",
        "    dtype = torch.FloatTensor\n",
        "    inputs[k] = dtype(v)\n",
        "\n",
        "  if return_max_len:\n",
        "    return inputs, max_lengths\n",
        "  return inputs"
      ],
      "metadata": {
        "id": "F3NBT5XfIpxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_datasets(targets, audios, train_percent = 0.9, cv = False, train_idx = [], test_idx = [], show_result = False):\n",
        "\n",
        "  \"\"\"Generate splits\"\"\"\n",
        "  np.random.seed(123)\n",
        "\n",
        "  #Cross validation / Force split train set to train & val\n",
        "  if cv == True:\n",
        "    assert len(train_idx) > 0\n",
        "    #assert len(test_idx) > 0 #If test_idx = none -> split only train/val, test with all data\n",
        "\n",
        "    idx_train = np.random.choice(train_idx, int(len(train_idx) * train_percent), replace = False)\n",
        "    idx_val = [x for x in train_idx if x not in idx_train]\n",
        "    idx_test = test_idx\n",
        "\n",
        "  #Train and test only\n",
        "  elif (val_size == 0) and (test_size != 0):\n",
        "    idx_train = np.random.choice(np.arange(train_size), int(train_size * train_percent), replace = False)\n",
        "    idx_val = [x for x in np.arange(train_size) if x not in idx_train]\n",
        "    idx_test = np.arange(train_size, len(all_targets))\n",
        "\n",
        "  #Train, val, and test\n",
        "  elif val_size != 0:\n",
        "    idx_train = np.arange(0, train_size)\n",
        "    idx_val = np.arange(train_size, train_size + val_size)\n",
        "    idx_test = np.arange(train_size + val_size, len(all_targets))\n",
        "  else:\n",
        "    raise Exception(\"Unknown split.\")\n",
        "\n",
        "  print(\"Data Loader split:\")\n",
        "  print(\"  - Train:\", len(idx_train))\n",
        "  print(\"  - Val:\", len(idx_val))\n",
        "  print(\"  - Test:\", len(idx_test))\n",
        "\n",
        "  \"\"\"Generate inputs\"\"\"\n",
        "  print(\"Training inputs\")\n",
        "  train_inputs, max_lengths = build_inputs([audios[i] for i in idx_train], return_max_len = True) #Extract max_lengths to implement on val and test sets\n",
        "  print(\"Validation inputs\")\n",
        "  val_inputs = build_inputs([audios[i] for i in idx_val], max_lengths = max_lengths)\n",
        "  print(\"Test inputs\")\n",
        "  test_inputs = build_inputs([audios[i] for i in idx_test], max_lengths = max_lengths) if len(idx_test) > 0 else []\n",
        "\n",
        "  \"\"\"Prepare loaders\"\"\"\n",
        "\n",
        "  train_targets = torch.LongTensor(targets[idx_train])\n",
        "  val_targets = torch.LongTensor(targets[idx_val])\n",
        "  test_targets = torch.LongTensor(targets[idx_test])\n",
        "\n",
        "  #Shuffle is turned off to ensure multiple loaders loading the same samples in the same order\n",
        "  train_loader = torch.utils.data.DataLoader(CustomDataset(train_inputs, train_targets, idx_train), shuffle=True, batch_size = BATCH_SIZE)\n",
        "  val_loader = torch.utils.data.DataLoader(CustomDataset(val_inputs, val_targets, idx_val), shuffle = True, batch_size = BATCH_SIZE)\n",
        "  test_loader = torch.utils.data.DataLoader(CustomDataset(test_inputs, test_targets, idx_test), shuffle = False, batch_size = BATCH_SIZE)\n",
        "\n",
        "  if len(test_targets) == 0:\n",
        "    return train_loader, val_loader\n",
        "  return train_loader, val_loader, test_loader\n"
      ],
      "metadata": {
        "id": "aCLVpqAfOmnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(show_result = True, epochs = 3, early_stop = 10, scheduler = None):\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "    for epoch in range(epochs):\n",
        "        t = time.time()\n",
        "        model.train()\n",
        "\n",
        "        f1_batch_train = []\n",
        "        acc_batch_train = []\n",
        "        loss_batch_train = []\n",
        "        for batch in train_loader:\n",
        "          targets = batch[\"labels\"].to(device)\n",
        "          input_args = {k: v.to(device) for k, v in batch.items() if k != \"ids\"}\n",
        "\n",
        "          output = model(**input_args)\n",
        "          loss_train = criterion(output[\"logits\"], targets)\n",
        "          optimizer.zero_grad()\n",
        "          loss_train.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          loss_batch_train.append(loss_train.item())\n",
        "          acc_batch_train.append(cal_accuracy(output[\"logits\"], targets))\n",
        "          f1_batch_train.append(f1_score(targets.cpu(), torch.argmax(output[\"logits\"].cpu(), axis = -1), average = \"weighted\"))\n",
        "\n",
        "        train_loss.append(np.mean(loss_batch_train))\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "          loss_batch_val = []\n",
        "          acc_batch_val = []\n",
        "          f1_batch_val = []\n",
        "          for batch in val_loader:\n",
        "            targets = batch[\"labels\"].to(device)\n",
        "            input_args = {k: v.to(device) for k, v in batch.items() if k != \"ids\"}\n",
        "\n",
        "            output = model(**input_args)\n",
        "            loss_val = criterion(output[\"logits\"], targets)\n",
        "\n",
        "            loss_batch_val.append(loss_val.item())\n",
        "            acc_batch_val.append(cal_accuracy(output[\"logits\"], targets))\n",
        "            f1_batch_val.append(f1_score(targets.cpu(), torch.argmax(output[\"logits\"].cpu(), axis = -1), average = \"weighted\"))\n",
        "\n",
        "        val_loss.append(np.mean(loss_batch_val))\n",
        "\n",
        "        if scheduler != None:\n",
        "          scheduler.step(np.mean(loss_batch_val))\n",
        "\n",
        "        if show_result:\n",
        "            print(  'Epoch: {:04d}'.format(epoch+1),\n",
        "                    'loss_train: {:.4f}'.format(np.mean(loss_batch_train)),\n",
        "                    'acc_train: {:.4f}'.format(np.mean(acc_batch_train)),\n",
        "                    'f1w_train: {:.4f}'.format(np.mean(f1_batch_train)),\n",
        "                    'loss_val: {:.4f}'.format(np.mean(loss_batch_val)),\n",
        "                    'acc_val: {:.4f}'.format(np.mean(acc_batch_val)),\n",
        "                    'f1w_val: {:.4f}'.format(np.mean(f1_batch_val)),\n",
        "                    'time: {:.4f}s'.format(time.time() - t),\n",
        "                    'lr:', optimizer.param_groups[0][\"lr\"], flush = True)\n",
        "\n",
        "        if early_stop != None and early_stop != 0 and epoch > early_stop and np.min(val_loss[-early_stop:]) > np.min(val_loss[:-early_stop]) :\n",
        "            if show_result:\n",
        "                print(\"Early Stopping...\")\n",
        "            break\n",
        "\n",
        "    plt.title(\"%s\" % (checkpoint))\n",
        "    plt.plot(train_loss, label = \"train\")\n",
        "    plt.plot(val_loss, label = \"Val\")\n",
        "    plt.legend()\n",
        "    plt.savefig(filePath_trainplot)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "NQzW2gnAQVpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "HrcHaEmPQW6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, hamming_loss, roc_auc_score\n",
        "from scipy.special import softmax"
      ],
      "metadata": {
        "id": "OlDnGxBnQaIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_accuracy(predictions,labels):\n",
        "    pred = torch.argmax(predictions,-1).cpu().tolist()\n",
        "    lab = labels.cpu().tolist()\n",
        "    cor = 0\n",
        "    for i in range(len(pred)):\n",
        "        if pred[i] == lab[i]:\n",
        "            cor += 1\n",
        "    return cor/len(pred)"
      ],
      "metadata": {
        "id": "REI0-1tlQX87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tuning"
      ],
      "metadata": {
        "id": "6tHuG5etQbJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna"
      ],
      "metadata": {
        "id": "XT7rncaPQjSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "\n",
        "  tune_dropout = trial.suggest_categorical(\"dropout\", [0.01, 0.05, 0.1, 0.5])\n",
        "\n",
        "  use_scheduler = False\n",
        "  if checkpoint == \"MIT/ast-finetuned-audioset-10-10-0.4593\":\n",
        "    tune_lr = trial.suggest_categorical(\"learning_rate\", [1e-03, 1e-04, 1e-05, 5e-05])\n",
        "    tune_layers = trial.suggest_int(\"num_hidden_layers\", 2, 12, 2)\n",
        "    tune_heads = trial.suggest_categorical(\"num_attention_heads\", [ 2,  3,  4,  6,  8, 12]) #choose num heads % 768\n",
        "    tune_patience = trial.suggest_int(\"scheduler_patience\", 2, 5)\n",
        "    tune_factor = trial.suggest_categorical(\"scheduler_factor\", [0.1, 0.5])\n",
        "\n",
        "    args = {\n",
        "            \"num_hidden_layers\": tune_layers,\n",
        "            \"num_attention_heads\": tune_heads,\n",
        "            \"hidden_dropout_prob\": tune_dropout,\n",
        "            \"attention_dropout_prob\": tune_dropout,\n",
        "            \"max_length\": MAX_LENGTH\n",
        "            }\n",
        "\n",
        "    tune_model = get_model(checkpoint, num_class, args).to(device)\n",
        "    tune_decay = 0\n",
        "    tune_epochs = 25\n",
        "    early_stop = 5\n",
        "    use_scheduler = True\n",
        "\n",
        "  else:\n",
        "    raise Exception(\"Unsupported checkpoint\")\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(tune_model.parameters(), lr = tune_lr, weight_decay = tune_decay)\n",
        "  if use_scheduler:\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=tune_factor, patience=tune_patience, verbose=True)\n",
        "\n",
        "  #Training\n",
        "  val_loss = []\n",
        "  for epoch in range(tune_epochs):\n",
        "    t = time.time()\n",
        "    tune_model.train()\n",
        "\n",
        "    for batch in train_loader:\n",
        "      targets = batch[\"labels\"].to(device)\n",
        "      input_args = {k: v.to(device) for k, v in batch.items() if k != \"ids\"}\n",
        "\n",
        "      output = tune_model(**input_args)\n",
        "      loss_train = criterion(output[\"logits\"], targets)\n",
        "      optimizer.zero_grad()\n",
        "      loss_train.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    tune_model.eval()\n",
        "    with torch.no_grad():\n",
        "      f1_batch_val = []\n",
        "      loss_batch_val = []\n",
        "      for batch in val_loader:\n",
        "        targets = batch[\"labels\"].to(device)\n",
        "        input_args = {k: v.to(device) for k, v in batch.items() if k != \"ids\"}\n",
        "\n",
        "        output = tune_model(**input_args)\n",
        "        loss_val = criterion(output[\"logits\"], targets)\n",
        "\n",
        "        loss_batch_val.append(loss_val.item())\n",
        "        f1_batch_val.append(f1_score(targets.cpu(), torch.argmax(output[\"logits\"].cpu(), axis = -1), average = \"weighted\"))\n",
        "\n",
        "    val_loss.append(np.mean(loss_batch_val))\n",
        "    f1_val = np.mean(f1_batch_val)\n",
        "\n",
        "    scheduler.step(np.mean(loss_batch_val))\n",
        "\n",
        "    #Record metric\n",
        "    trial.report(f1_val, epoch)\n",
        "\n",
        "    if early_stop != None and early_stop != 0 and epoch > early_stop and np.min(val_loss[-early_stop:]) > np.min(val_loss[:-early_stop]) :\n",
        "      break\n",
        "\n",
        "    # Handle pruning based on the intermediate value.\n",
        "    if trial.should_prune():\n",
        "        raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "  return f1_val"
      ],
      "metadata": {
        "id": "fJe0cH9dQgtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_parameters(n_trials = 50):\n",
        "\n",
        "  study = optuna.create_study(direction = \"maximize\")\n",
        "\n",
        "  if checkpoint == \"MIT/ast-finetuned-audioset-10-10-0.4593\":\n",
        "    study.enqueue_trial({\"dropout\": 0.5,  #default parameters\n",
        "                        \"num_hidden_layers\": 12,\n",
        "                        \"num_attention_heads\": 12,\n",
        "                        \"learning_rate\": 5e-05,\n",
        "                        \"weight_decay\": 0,\n",
        "                        \"scheduler_patience\": 5,\n",
        "                        \"scheduler_factor\": 0.5})\n",
        "  else:\n",
        "    raise Exception(\"Unsupported checkpoint\")\n",
        "  study.optimize(objective, n_trials = n_trials)\n",
        "\n",
        "  return study"
      ],
      "metadata": {
        "id": "Ahte8ODaQl5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AST"
      ],
      "metadata": {
        "id": "8IzyGSlmQoeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
        "filePath_model = \"%s/%s.pt\" % (SAVE_PATH, checkpoint.replace(\"/\", \"_\"))\n",
        "filePath_log = \"%s/%s.log\" % (SAVE_PATH, checkpoint.replace(\"/\", \"_\"))\n",
        "filePath_trainplot = \"%s/%s.png\" % (SAVE_PATH, checkpoint.replace(\"/\", \"_\"))\n",
        "BATCH_SIZE = 32\n",
        "MAX_LENGTH = 512"
      ],
      "metadata": {
        "id": "u56khLWmQpTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, val_loader = prepare_datasets(all_targets, all_audio, cv = True, train_idx = np.arange(len(data.texts)))"
      ],
      "metadata": {
        "id": "9GVlqoUdQuVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tuning"
      ],
      "metadata": {
        "id": "Kh0a-AfaQx6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = datetime.now()\n",
        "print(\"Tuning...\", flush = True)\n",
        "study = tune_parameters(n_trials = 2)\n",
        "print(\"Total tuning time: %s\\n\" % (datetime.now() - start), flush = True)\n",
        "\n",
        "best_trial = study.best_trial\n",
        "best_params = best_trial.params\n",
        "\n",
        "print(\"BEST:\", best_trial.value)\n",
        "print(\"Params:\")\n",
        "for key, value in best_params.items():\n",
        "  print(\"    {}: {}\".format(key, value))"
      ],
      "metadata": {
        "id": "IA-VpLu4QzOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training"
      ],
      "metadata": {
        "id": "3qdM7FlOQ2Qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\"num_hidden_layers\": best_params[\"num_hidden_layers\"],\n",
        "        \"num_attention_heads\": best_params[\"num_attention_heads\"],\n",
        "        \"hidden_dropout_prob\": best_params[\"dropout\"],\n",
        "        \"attention_dropout_prob\": best_params[\"dropout\"],\n",
        "        \"max_length\": MAX_LENGTH}\n",
        "\n",
        "model = get_model(checkpoint, num_class, args).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr = best_params[\"learning_rate\"]) #, weight_decay = best_params[\"weight_decay\"])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=best_params[\"scheduler_factor\"], patience=best_params[\"scheduler_patience\"], verbose=True)\n",
        "\n",
        "print(\"=\" * 20, \"MODEL CONFIG\", \"=\" * 20, flush = True)\n",
        "print(model)\n",
        "\n",
        "train_model(epochs = 25, early_stop = 5, scheduler = scheduler)\n",
        "\n",
        "torch.save(model, filePath_model) #Save model\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "v-rAeIc4Q1CB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}