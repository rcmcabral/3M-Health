{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "PZuPemJQFMw1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxluQZ1HE1lW"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "from tabulate import tabulate\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, hamming_loss, roc_auc_score\n",
        "from scipy.special import softmax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "id": "7aHPbeCdFRcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set Variables"
      ],
      "metadata": {
        "id": "j3PyFmvbJAUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"_MODELS\", exist_ok = True) #Folder to save all models\n",
        "\n",
        "#Path to emotion embeddings\n",
        "#File must contain vocabulary map and weights\n",
        "emoEmbPath = \"\"\n",
        "\n",
        "MAX_LENGTH = 256\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "cJonowRaJBka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Data"
      ],
      "metadata": {
        "id": "-ncVr7DoIonL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0ZrSuoxNmdM"
      },
      "outputs": [],
      "source": [
        "#Data Preparation\n",
        "original_train_sentences = []\n",
        "original_val_sentences = []\n",
        "original_test_sentences = []\n",
        "\n",
        "original_train_labels = []\n",
        "original_val_labels = []\n",
        "original_test_labels = []\n",
        "\n",
        "assert len(original_train_sentences) == len(original_train_labels)\n",
        "assert len(original_val_sentences) == len(original_val_labels)\n",
        "assert len(original_test_sentences) == len(original_test_labels)\n",
        "\n",
        "train_size = len(original_train_sentences)\n",
        "val_size = len(original_val_sentences)\n",
        "test_size = len(original_test_sentences)\n",
        "\n",
        "train_idx = np.arange(0, train_size)\n",
        "val_idx = np.arange(train_size, train_size + val_size)\n",
        "test_idx = np.arange(train_size + val_size, train_size + val_size + test_size)\n",
        "\n",
        "all_sentences = np.array(original_train_sentences + original_val_sentences + original_test_sentences)\n",
        "all_labels = np.array(original_train_labels + original_val_labels + original_test_labels)\n",
        "\n",
        "#Label Encoding\n",
        "unique_labels = np.unique(original_train_labels)\n",
        "num_class = len(unique_labels)\n",
        "\n",
        "lEnc = LabelEncoder()\n",
        "lEnc.fit(unique_labels)\n",
        "\n",
        "print(unique_labels)\n",
        "print(lEnc.transform(unique_labels))\n",
        "\n",
        "all_targets = lEnc.transform(all_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3QSsuKsDaz4"
      },
      "source": [
        "#Load Resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKfgCH5aDQuG"
      },
      "outputs": [],
      "source": [
        "#Load list of emoticons\n",
        "#Source: https://c.r74n.com/faces\n",
        "\n",
        "with open(\"TextEmoticonList.txt\", \"r\") as file:\n",
        "  emoticonList = file.read().split(\"\\n\")\n",
        "\n",
        "#Remove emoticons with spaces in-between\n",
        "emoticonList = [emoticon for emoticon in emoticonList if len(emoticon.split(\" \")) == 1]\n",
        "\n",
        "#Remove one character emoticons\n",
        "emoticonList = [emoticon for emoticon in emoticonList if len(emoticon) > 1]\n",
        "\n",
        "print(len(emoticonList))\n",
        "print(emoticonList[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Knh2n_bAZOrF"
      },
      "outputs": [],
      "source": [
        "#Load list of emojis\n",
        "#Source: https://www.airtable.com/universe/exphjm5ifnV0bX4Kb/emojis-database?explore=true\n",
        "\n",
        "emojiList = pd.read_csv(\"Emojis-Grid view.csv\")\n",
        "emojiList = emojiList[emojiList[\"Emoji\"] != \"C\"]\n",
        "emojiList = emojiList[\"Emoji\"].tolist()\n",
        "\n",
        "#Unicode versions\n",
        "emojiList_uni = [emoji.encode('unicode-escape').decode('ASCII') for emoji in emojiList]\n",
        "\n",
        "print(len(emojiList))\n",
        "print(emojiList[:10])\n",
        "print(emojiList_uni[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WVujY7b0MpD"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text"
      ],
      "metadata": {
        "id": "E00tYga8oNC1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbws3oNj7nSP"
      },
      "outputs": [],
      "source": [
        "#FLAGS\n",
        "DEIDENTIFY = True     #Replace urls, emails, and usernames\n",
        "EMOPRESERVE = True    #Identify emojis/emoticons on text and skip text cleaning on them\n",
        "TEXTCLEAN = False     #Minimal cleaning of separating certain conjunctions\n",
        "TOKEN_TYPE = \"wp\"     #wp: word piece (BERT Tokenizer); ws: word split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D4_Mb-y0MpE"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "tokenURL = \"_URL_\"\n",
        "tokenEmail = \"_EMAIL_\"\n",
        "tokenUsername = \"_USER_\"\n",
        "reserveTokens = [tokenURL, tokenEmail, tokenUsername]\n",
        "\n",
        "#CLEANING PROCESS\n",
        "#- Include emojis and emoticons\n",
        "#- Replace url, email, and usernames with tokens\n",
        "#- Remove non-major puncutations and separate them from words with whitespaces\n",
        "#- Lowercase\n",
        "def preprocess_str(string):\n",
        "\n",
        "  #Preclean\n",
        "  if DEIDENTIFY:\n",
        "    string = re.sub(r\"https?://[^\\s]+\", tokenURL, string)              #Links\n",
        "    string = re.sub(r\"[\\w.+-]+@[\\w-]+\\.[\\w.-]+\", tokenEmail, string)   #Email\n",
        "    string = re.sub(r\"@[a-zA-Z0-9_]{2,}\", tokenUsername, string)       #Usernames\n",
        "\n",
        "  #Emoticon/Emoji split\n",
        "  tokens = [string]\n",
        "  if EMOPRESERVE:\n",
        "    allEmo = emoticonList + emojiList + emojiList_uni + reserveTokens\n",
        "    for emoticon in allEmo:\n",
        "      regEx = \"(^|\\s)\" + re.escape(emoticon) + \"(\\s|$)\" if emoticon.isalpha() else re.escape(emoticon)\n",
        "      if emoticon in string:\n",
        "        splits = []\n",
        "        for split in tokens:\n",
        "          splits.append(re.split(r\"(\" + regEx + \")\", split))\n",
        "        tokens = [y.strip() for x in splits for y in x if y != \"\"]\n",
        "\n",
        "  for idx in range(len(tokens)):\n",
        "    if EMOPRESERVE and tokens[idx] in allEmo: #Skip emoticons, emojis\n",
        "      continue\n",
        "\n",
        "    if TEXTCLEAN:\n",
        "      tokens[idx] = re.sub(r\"[^A-Za-z0-9(),!?\\.\\'\\`]\", \" \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\'s\", \" \\'s\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\'ve\", \" \\'ve\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"n\\'t\", \" n\\'t\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\'re\", \" \\'re\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\'d\", \" \\'d\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\'ll\", \" \\'ll\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\",\", \" , \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"!\", \" ! \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\(\", \" ( \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\)\", \" ) \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\?\", \" ? \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\.\", \" . \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\s{2,}\", \" \", tokens[idx])\n",
        "\n",
        "    #Lower case and strip by default\n",
        "    tokens[idx] = tokens[idx].lower().strip()\n",
        "\n",
        "  return \" \".join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjMJlTN60MpF"
      },
      "source": [
        "##Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KsklvB7KNwt"
      },
      "outputs": [],
      "source": [
        "def get_tokenizer(token_type, checkpoint = None):\n",
        "  if token_type.lower() == \"wp\":\n",
        "    if checkpoint in [None, \"bert-base-uncased\", \"custom/MM-EMOG-SenticNet\"]:\n",
        "      from transformers import BertTokenizer\n",
        "      tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    else:\n",
        "      from transformers import AutoTokenizer\n",
        "      tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "    if DEIDENTIFY:\n",
        "      tokenizer.add_tokens(reserveTokens)\n",
        "\n",
        "    if EMOPRESERVE:\n",
        "      #Add spaces to alpha emotions to avoid splitting words that commonly has them (ie \"omo\" in \"tomorrow\")\n",
        "      temp = [\" %s \" % x if x.isalpha() else x for x in emoticonList]\n",
        "      tokenizer.add_tokens(temp + emojiList + emojiList_uni)\n",
        "\n",
        "    return tokenizer\n",
        "  elif token_type.lower() == \"ws\":\n",
        "    return string.split()\n",
        "  else:\n",
        "    raise Exception(\"Unknown value for TOKEN_TYPE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models"
      ],
      "metadata": {
        "id": "mtWcKwnWFtfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pretrained LM"
      ],
      "metadata": {
        "id": "4ePu-XngFunM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "def get_plm(checkpoint, num_class, args = None):\n",
        "\n",
        "  if checkpoint in [\"bert-base-uncased\", \"roberta-base\", \"mental/mental-bert-base-uncased\"]:\n",
        "    config = AutoConfig.from_pretrained(checkpoint, num_labels = num_class)\n",
        "\n",
        "    if args != None:\n",
        "      config = AutoConfig.from_pretrained(checkpoint, num_labels = num_class, **args)\n",
        "    else:\n",
        "      config = AutoConfig.from_pretrained(checkpoint, num_labels = num_class)\n",
        "\n",
        "    return AutoModelForSequenceClassification.from_pretrained(checkpoint, config = config)\n",
        "  elif checkpoint.split(\"/\")[0] == \"custom\":\n",
        "    assert \"pt_weights\" in args\n",
        "    assert \"pt_weights_dim\" in args\n",
        "\n",
        "    return MLP(num_class = num_class, **args)\n",
        "  else:\n",
        "    raise Exception(\"Unknown checkpoint\")"
      ],
      "metadata": {
        "id": "QyBJXH9TFxs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MLP"
      ],
      "metadata": {
        "id": "oxQopGyQF4aV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, pt_weights, pt_weights_dim, num_class, num_layers, hidden_dim, dropout, actual_max):\n",
        "    super(MLP, self).__init__()\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    #Load Embeddings\n",
        "    self.embeddings = nn.Embedding.from_pretrained(pt_weights)\n",
        "\n",
        "    #MLP\n",
        "    if self.num_layers >= 2:\n",
        "      self.l1 = nn.Linear(actual_max * pt_weights_dim, hidden_dim)\n",
        "      self.r1 = nn.ReLU()\n",
        "      self.d1 = nn.Dropout(dropout)\n",
        "\n",
        "      moduleList = []\n",
        "      for _ in range(num_layers - 2):\n",
        "        moduleList.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "        moduleList.append(nn.ReLU())\n",
        "        moduleList.append(nn.Dropout(dropout))\n",
        "\n",
        "      self.mod_list = nn.ModuleList(moduleList)\n",
        "      self.lf = nn.Linear(hidden_dim, num_class)\n",
        "      self.rf = nn.ReLU()\n",
        "      self.df = nn.Dropout(dropout)\n",
        "    else:\n",
        "      self.l1 = nn.Linear(actual_max * pt_weights_dim, num_class)\n",
        "      self.r1 = nn.ReLU()\n",
        "      self.d1 = nn.Dropout(dropout)\n",
        "    # self.softmax = nn.Softmax()\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "\n",
        "    #Generate embeddings\n",
        "    features = self.embeddings(input_ids)      # embedded = [batch size, sent_len, emb dim]\n",
        "\n",
        "    # features = torch.cat((bert_features, mmemog_features), axis = -1)\n",
        "    x = features.view(features.shape[0], -1)  #Flatten\n",
        "\n",
        "    if self.num_layers >= 2:\n",
        "      x = self.l1(x)\n",
        "      x = self.r1(x)\n",
        "      x = self.d1(x)\n",
        "\n",
        "      for i in range(self.num_layers - 2):\n",
        "        x = self.mod_list[i](x)\n",
        "\n",
        "      x = self.lf(x)\n",
        "      x = self.rf(x)\n",
        "      x = self.df(x)\n",
        "    else:\n",
        "      x = self.l1(x)\n",
        "      x = self.r1(x)\n",
        "      x = self.d1(x)\n",
        "    # output = self.softmax(x)\n",
        "    output = {\"logits\": x}\n",
        "    return output"
      ],
      "metadata": {
        "id": "wyS9lB84F7Ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Functions"
      ],
      "metadata": {
        "id": "Cu2wQ7TeF8qT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Initialize Model"
      ],
      "metadata": {
        "id": "uZEhlp5tGBT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_accuracy(predictions,labels):\n",
        "    pred = torch.argmax(predictions,-1).cpu().tolist()\n",
        "    lab = labels.cpu().tolist()\n",
        "    cor = 0\n",
        "    for i in range(len(pred)):\n",
        "        if pred[i] == lab[i]:\n",
        "            cor += 1\n",
        "    return cor/len(pred)\n",
        "\n",
        "class BERTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.inputs.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def prepare_datasets(train_percent = 0.9, cv = False, train_idx = None, test_idx = None, show_result = False):\n",
        "\n",
        "  np.random.seed(123)\n",
        "  #Cross validation\n",
        "  if cv == True:\n",
        "    assert len(train_idx) > 0\n",
        "    #assert len(test_idx) > 0 #If test_idx = none -> split only train/val, test with all data\n",
        "\n",
        "    idx_train = np.random.choice(train_idx, int(len(train_idx) * train_percent), replace = False)\n",
        "    idx_val = [x for x in train_idx if x not in idx_train]\n",
        "    idx_test = test_idx\n",
        "\n",
        "  #Train and test only\n",
        "  elif (val_size == 0) and (test_size != 0):\n",
        "    idx_train = np.random.choice(np.arange(train_size), int(train_size * train_percent), replace = False)\n",
        "    idx_val = [x for x in np.arange(train_size) if x not in idx_train]\n",
        "    idx_test = np.arange(train_size, len(all_sentences))\n",
        "\n",
        "  #Train, val, and test\n",
        "  elif val_size != 0:\n",
        "    idx_train = np.arange(0, train_size)\n",
        "    idx_val = np.arange(train_size, train_size + val_size)\n",
        "    idx_test = np.arange(train_size + val_size, len(all_sentences))\n",
        "  else:\n",
        "    raise Exception(\"Unknown split.\")\n",
        "\n",
        "  print(\"Data Loader split\")\n",
        "  print(\"  - Train:\", len(idx_train))\n",
        "  print(\"  - Val:\", len(idx_val))\n",
        "  print(\"  - Test:\", len(idx_test) if idx_test != None else 0)\n",
        "\n",
        "  if type(all_input) == np.ndarray:\n",
        "    train_input = all_input[idx_train]\n",
        "    val_input = all_input[idx_val]\n",
        "    test_input = all_input[idx_test]\n",
        "  else:\n",
        "    train_input = {key: torch.LongTensor(value)[idx_train] for key, value in all_input.items()}\n",
        "    val_input = {key: torch.LongTensor(value)[idx_val] for key, value in all_input.items()}\n",
        "    test_input = {key: torch.LongTensor(value)[idx_test] for key, value in all_input.items()}\n",
        "\n",
        "  train_targets = torch.LongTensor(all_targets[idx_train])\n",
        "  val_targets = torch.LongTensor(all_targets[idx_val])\n",
        "  test_targets = torch.LongTensor(all_targets[idx_test])\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(BERTDataset(train_input, train_targets), shuffle=True, batch_size = BATCH_SIZE)\n",
        "  val_loader = torch.utils.data.DataLoader(BERTDataset(val_input, val_targets), shuffle = True, batch_size = BATCH_SIZE)\n",
        "  test_loader = torch.utils.data.DataLoader(BERTDataset(test_input, test_targets), shuffle = False, batch_size = BATCH_SIZE)\n",
        "  print(\"Batch size:\", BATCH_SIZE)\n",
        "\n",
        "  if len(test_targets) == 0:\n",
        "    return train_loader, val_loader\n",
        "  return train_loader, val_loader, test_loader\n",
        "\n",
        "def getEmoEmbeddings():\n",
        "  with open(emoEmbPath, \"rb\") as file:\n",
        "    content = pickle.load(file)\n",
        "\n",
        "  return content[\"vocab_map\"], content[\"weights\"]"
      ],
      "metadata": {
        "id": "cq9lXHqgF8FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train"
      ],
      "metadata": {
        "id": "25-IpubIGlO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_input(batch):\n",
        "  if checkpoint.split(\"/\")[0] != \"custom\":\n",
        "    input_ids = batch[\"input_ids\"].to(device)\n",
        "    targets = batch[\"labels\"].to(device)\n",
        "    input_args = {\"input_ids\": input_ids,\n",
        "                  \"labels\": targets}\n",
        "\n",
        "    if \"attention_mask\" in batch.keys():\n",
        "      attention_mask = batch[\"attention_mask\"].to(device)\n",
        "      input_args[\"attention_mask\"] = attention_mask\n",
        "    if \"token_type_ids\" in batch.keys():\n",
        "      token_type_ids = batch[\"token_type_ids\"].to(device)\n",
        "      input_args[\"token_type_ids\"] = token_type_ids\n",
        "  else:\n",
        "    input_args = {\"input_ids\": batch[\"input_ids\"].to(device)}\n",
        "\n",
        "  return input_args"
      ],
      "metadata": {
        "id": "KhshYv-DGnrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch.optim as optim\n",
        "\n",
        "def train_model(show_result = True, epochs = 3, early_stop = 10):\n",
        "    val_loss = []\n",
        "    for epoch in range(epochs):\n",
        "        t = time.time()\n",
        "        model.train()\n",
        "\n",
        "        f1_batch_train = []\n",
        "        acc_batch_train = []\n",
        "        loss_batch_train = []\n",
        "        for batch in train_loader:\n",
        "          targets = batch[\"labels\"].to(device)\n",
        "          input_args = collate_input(batch)\n",
        "\n",
        "          output = model(**input_args)\n",
        "          loss_train = criterion(output[\"logits\"], targets)\n",
        "          optimizer.zero_grad()\n",
        "          loss_train.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          loss_batch_train.append(loss_train.item())\n",
        "          acc_batch_train.append(cal_accuracy(output[\"logits\"], targets))\n",
        "          f1_batch_train.append(f1_score(targets.cpu(), torch.argmax(output[\"logits\"].cpu(), axis = -1), average = \"weighted\"))\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "          loss_batch_val = []\n",
        "          acc_batch_val = []\n",
        "          f1_batch_val = []\n",
        "          for batch in val_loader:\n",
        "            targets = batch[\"labels\"].to(device)\n",
        "            input_args = collate_input(batch)\n",
        "\n",
        "            output = model(**input_args)\n",
        "            loss_val = criterion(output[\"logits\"], targets)\n",
        "\n",
        "            loss_batch_val.append(loss_val.item())\n",
        "            acc_batch_val.append(cal_accuracy(output[\"logits\"], targets))\n",
        "            f1_batch_val.append(f1_score(targets.cpu(), torch.argmax(output[\"logits\"].cpu(), axis = -1), average = \"weighted\"))\n",
        "\n",
        "        val_loss.append(np.mean(loss_batch_val))\n",
        "\n",
        "        if show_result:\n",
        "            print(  'Epoch: {:04d}'.format(epoch+1),\n",
        "                    'loss_train: {:.4f}'.format(np.mean(loss_batch_train)),\n",
        "                    'acc_train: {:.4f}'.format(np.mean(acc_batch_train)),\n",
        "                    'f1w_train: {:.4f}'.format(np.mean(f1_batch_train)),\n",
        "                    'loss_val: {:.4f}'.format(np.mean(loss_batch_val)),\n",
        "                    'acc_val: {:.4f}'.format(np.mean(acc_batch_val)),\n",
        "                    'f1w_val: {:.4f}'.format(np.mean(f1_batch_val)),\n",
        "                    'time: {:.4f}s'.format(time.time() - t), flush = True)\n",
        "\n",
        "        if early_stop != None and epoch > early_stop and np.min(val_loss[-early_stop:]) > np.min(val_loss[:-early_stop]) :\n",
        "            if show_result:\n",
        "                print(\"Early Stopping...\")\n",
        "                plt.plot(val_loss)\n",
        "                plt.show()\n",
        "            break"
      ],
      "metadata": {
        "id": "6YwT_Pm2GqvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tuning"
      ],
      "metadata": {
        "id": "i79JjLIpG1qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "  tune_dropout = trial.suggest_categorical(\"dropout\", [0.01, 0.05, 0.1, 0.5])\n",
        "  tune_decay = trial.suggest_categorical(\"weight_decay\", [0, 0.01, 0.1])\n",
        "\n",
        "  if checkpoint.split(\"/\")[0] != \"custom\":\n",
        "    tune_lr = trial.suggest_categorical(\"learning_rate\", [1e-04, 1e-05, 2e-05, 3e-05, 4e-05, 5e-05])\n",
        "    tune_epochs = trial.suggest_int(\"num_epochs\", 2, 5)\n",
        "    tune_layers = trial.suggest_int(\"num_hidden_layers\", 2, 12, 2)\n",
        "    tune_heads = trial.suggest_categorical(\"num_attention_heads\", [ 2,  3,  4,  6,  8, 12]) #choose num heads % 768\n",
        "    early_stop = None\n",
        "\n",
        "    if checkpoint.split(\"/\")[0] == \"medicalai\":\n",
        "      args = {\"num_hidden_layers\": tune_layers,\n",
        "              \"num_attention_heads\": tune_heads,\n",
        "              \"dropout\": tune_dropout,\n",
        "              \"attention_dropout\": tune_dropout,\n",
        "              \"hidden_act\": \"relu\"}\n",
        "    else:\n",
        "      args = {\n",
        "              \"num_hidden_layers\": tune_layers,\n",
        "              \"num_attention_heads\": tune_heads,\n",
        "              \"hidden_dropout_prob\": tune_dropout,\n",
        "              \"attention_dropout_prob\": tune_dropout,\n",
        "              \"hidden_act\": \"relu\"\n",
        "              }\n",
        "    tune_model = get_plm(checkpoint, num_class, args).to(device)\n",
        "    tune_model.resize_token_embeddings(len(tokenizer))         #Resize vocab for added emojis and reserved tokens\n",
        "  else: #MLP default\n",
        "    tune_layers = trial.suggest_int(\"num_hidden_layers\", 2, 5)\n",
        "    tune_lr = trial.suggest_categorical(\"learning_rate\", [1e-03,1e-04, 1e-05])\n",
        "    tune_hidden_dim = trial.suggest_int(\"hidden_dim\", 100, 500, 100)\n",
        "\n",
        "    tune_epochs = 100\n",
        "    early_stop = 10\n",
        "\n",
        "    args = {\"pt_weights\": pt_weights,\n",
        "            \"pt_weights_dim\": pt_weights_dim,\n",
        "            \"num_layers\": tune_layers,\n",
        "            \"hidden_dim\": tune_hidden_dim,\n",
        "            \"dropout\": tune_dropout,\n",
        "            \"actual_max\": actual_max}\n",
        "    tune_model = get_plm(checkpoint, num_class, args).to(device)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(tune_model.parameters(), lr = tune_lr, weight_decay = tune_decay)\n",
        "\n",
        "  #Training\n",
        "  val_loss = []\n",
        "  for epoch in range(tune_epochs):\n",
        "    t = time.time()\n",
        "    tune_model.train()\n",
        "\n",
        "    for batch in train_loader:\n",
        "      targets = batch[\"labels\"].to(device)\n",
        "      input_args = collate_input(batch)\n",
        "\n",
        "      output = tune_model(**input_args)\n",
        "      loss_train = criterion(output[\"logits\"], targets)\n",
        "      optimizer.zero_grad()\n",
        "      loss_train.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    tune_model.eval()\n",
        "    with torch.no_grad():\n",
        "      f1_batch_val = []\n",
        "      loss_batch_val = []\n",
        "      for batch in val_loader:\n",
        "        targets = batch[\"labels\"].to(device)\n",
        "        input_args = collate_input(batch)\n",
        "\n",
        "        output = tune_model(**input_args)\n",
        "        loss_val = criterion(output[\"logits\"], targets)\n",
        "\n",
        "        loss_batch_val.append(loss_val.item())\n",
        "        f1_batch_val.append(f1_score(targets.cpu(), torch.argmax(output[\"logits\"].cpu(), axis = -1), average = \"weighted\"))\n",
        "\n",
        "    val_loss.append(np.mean(loss_batch_val))\n",
        "    f1_val = np.mean(f1_batch_val)\n",
        "\n",
        "    #Record metric\n",
        "    trial.report(f1_val, epoch)\n",
        "\n",
        "    if early_stop != None and epoch > early_stop and np.min(val_loss[-early_stop:]) > np.min(val_loss[:-early_stop]) :\n",
        "      break\n",
        "\n",
        "    # Handle pruning based on the intermediate value.\n",
        "    if trial.should_prune():\n",
        "        raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "  return f1_val"
      ],
      "metadata": {
        "id": "_JRM25maG5fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_parameters(n_trials = 50):\n",
        "\n",
        "  study = optuna.create_study(direction = \"maximize\")\n",
        "\n",
        "  if checkpoint.split(\"/\")[0] != \"custom\":\n",
        "    study.enqueue_trial({\"dropout\": 0.1,  #default parameters\n",
        "                        \"num_hidden_layers\": 12,\n",
        "                        \"num_attention_heads\": 12,\n",
        "                        \"learning_rate\": 1e-05,\n",
        "                        \"weight_decay\": 0,\n",
        "                        \"num_epochs\": 3})\n",
        "  else:\n",
        "    study.enqueue_trial({\"dropout\": 0.1,  #default parameters\n",
        "                        \"num_hidden_layers\": 3,\n",
        "                        \"learning_rate\": 1e-05,\n",
        "                        \"weight_decay\": 0,\n",
        "                        \"hidden_dim\": 200})\n",
        "  study.optimize(objective, n_trials = n_trials)\n",
        "\n",
        "  return study"
      ],
      "metadata": {
        "id": "eDbEhcxkG9pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train Teachers"
      ],
      "metadata": {
        "id": "I-MdJStmJqkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_sentences = [preprocess_str(sent) for sent in tqdm(all_sentences)]"
      ],
      "metadata": {
        "id": "iwheTMEDNOZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MMEMOG"
      ],
      "metadata": {
        "id": "QCWPpLOOJLTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "print(\"=\" * 50)\n",
        "print(\"=\" * 20, \"MM-EMOG\", \"=\" * 20)\n",
        "print(\"=\" * 50)\n",
        "print()\n",
        "\n",
        "#SETUP\n",
        "checkpoint = \"custom/MM-EMOG-SenticNet\"\n",
        "filePath_model = \"_MODELS/%s.pt\" % ( checkpoint.replace(\"/\", \"_\"))\n",
        "\n",
        "#Load embeddings\n",
        "mmemog_map, mmemog_weights = getEmoEmbeddings()\n",
        "\n",
        "#Build tokenizer\n",
        "tokenizer = get_tokenizer(\"wp\", \"bert-base-uncased\")\n",
        "tokenizer.add_tokens(mmemog_map.keys())\n",
        "\n",
        "#Align tokenizer and mmemog weights\n",
        "np.random.seed(123)\n",
        "pt_weights_dim = mmemog_weights.shape[-1]\n",
        "pt_weights = np.random.random((max(len(tokenizer.get_vocab()), len(mmemog_map.keys())), pt_weights_dim))\n",
        "\n",
        "for key, val in tokenizer.get_vocab().items():\n",
        "  if key in mmemog_map:\n",
        "    pt_weights[val] = mmemog_weights[mmemog_map[key]]\n",
        "  # else keep randomly generated weights\n",
        "pt_weights = torch.FloatTensor(pt_weights).to(device)\n",
        "assert pt_weights.shape[0] == len(tokenizer.get_vocab())\n",
        "\n",
        "#Create input\n",
        "actual_max = min(MAX_LENGTH, max([len(tokenizer.tokenize(x)) for x in clean_sentences]))\n",
        "all_input = tokenizer(clean_sentences, padding = True, truncation = True, max_length = actual_max)\n",
        "print(\"Actual max length:\", actual_max)\n",
        "\n",
        "#Create loaders\n",
        "train_loader, val_loader, _ = prepare_datasets(cv = True, train_idx = np.arange(len(clean_sentences)))\n",
        "\n",
        "#TUNE\n",
        "start = datetime.now()\n",
        "print(\"Tuning...\", flush = True)\n",
        "study = tune_parameters(50)\n",
        "print(\"Total tuning time: %s\\n\" % (datetime.now() - start), flush = True)\n",
        "\n",
        "best_trial = study.best_trial\n",
        "best_params = best_trial.params\n",
        "print(\"BEST:\", best_trial.value)\n",
        "print(\"Params:\")\n",
        "for key, value in best_params.items():\n",
        "  print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "#TRAIN\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "args = {\"pt_weights\": pt_weights,\n",
        "        \"pt_weights_dim\": pt_weights_dim,\n",
        "        \"num_layers\": best_params[\"num_hidden_layers\"],\n",
        "        \"hidden_dim\": best_params[\"hidden_dim\"],\n",
        "        \"dropout\": best_params[\"dropout\"],\n",
        "        \"actual_max\": actual_max}\n",
        "\n",
        "model = get_plm(checkpoint, num_class, args).to(device)\n",
        "# model.resize_token_embeddings(len(tokenizer))         #Resize vocab for added emojis and reserved tokens\n",
        "optimizer = optim.Adam(model.parameters(), lr = best_params[\"learning_rate\"], weight_decay = best_params[\"weight_decay\"])\n",
        "\n",
        "print(\"=\" * 20, \"MODEL CONFIG\", \"=\" * 20)\n",
        "print(model)\n",
        "\n",
        "train_model(epochs = 100, early_stop = 10)\n",
        "\n",
        "torch.save(model, filePath_model) #Save model"
      ],
      "metadata": {
        "id": "K4Hz0_TbJXKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BERT"
      ],
      "metadata": {
        "id": "vYOEeLPsJox7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "print(\"=\" * 50)\n",
        "print(\"=\" * 20, \"BERT\", \"=\" * 20)\n",
        "print(\"=\" * 50)\n",
        "print()\n",
        "\n",
        "#SETUP\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "filePath_model = \"_MODELS/%s.pt\" % ( checkpoint.replace(\"/\", \"_\"))\n",
        "\n",
        "tokenizer = get_tokenizer(\"wp\", checkpoint)\n",
        "\n",
        "#Create input\n",
        "actual_max = min(MAX_LENGTH, max([len(tokenizer.tokenize(x)) for x in clean_sentences]))\n",
        "all_input = tokenizer(clean_sentences, padding = True, truncation = True, max_length = actual_max)\n",
        "print(\"Actual max length:\", actual_max)\n",
        "\n",
        "#Create loaders\n",
        "train_loader, val_loader, _ = prepare_datasets(cv = True, train_idx = np.arange(len(clean_sentences)))\n",
        "\n",
        "#TUNE\n",
        "start = datetime.now()\n",
        "print(\"Tuning...\", flush = True)\n",
        "study = tune_parameters(50)\n",
        "print(\"Total tuning time: %s\\n\" % (datetime.now() - start), flush = True)\n",
        "\n",
        "best_trial = study.best_trial\n",
        "best_params = best_trial.params\n",
        "print(\"BEST:\", best_trial.value)\n",
        "print(\"Params:\")\n",
        "for key, value in best_params.items():\n",
        "  print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "#TRAIN\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "args = {\"num_hidden_layers\": best_params[\"num_hidden_layers\"],\n",
        "        \"num_attention_heads\": best_params[\"num_attention_heads\"],\n",
        "        \"hidden_dropout_prob\": best_params[\"dropout\"],\n",
        "        \"attention_dropout_prob\": best_params[\"dropout\"],\n",
        "        \"hidden_act\": \"relu\"}\n",
        "\n",
        "model = get_plm(checkpoint, num_class, args).to(device)\n",
        "model.resize_token_embeddings(len(tokenizer))         #Resize vocab for added emojis and reserved tokens\n",
        "optimizer = optim.Adam(model.parameters(), lr = best_params[\"learning_rate\"], weight_decay = best_params[\"weight_decay\"])\n",
        "\n",
        "print(\"=\" * 20, \"MODEL CONFIG\", \"=\" * 20)\n",
        "print(model.config)\n",
        "\n",
        "train_model(epochs = best_params[\"num_epochs\"])\n",
        "\n",
        "torch.save(model, filePath_model) #Save model\n",
        "#uploadFile(filePath_model, filePath_model.split(\"/\")[-1])"
      ],
      "metadata": {
        "id": "ssSslwXPJupL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RoBERTa"
      ],
      "metadata": {
        "id": "uc3yI-WLJ9US"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "print(\"=\" * 50)\n",
        "print(\"=\" * 20, \"RoBERTa\", \"=\" * 20)\n",
        "print(\"=\" * 50)\n",
        "print()\n",
        "\n",
        "#SETUP\n",
        "checkpoint = \"roberta-base\"\n",
        "filePath_model = \"_MODELS/%s.pt\" % ( checkpoint.replace(\"/\", \"_\"))\n",
        "\n",
        "tokenizer = get_tokenizer(\"wp\", checkpoint)\n",
        "\n",
        "#Create input\n",
        "actual_max = min(MAX_LENGTH, max([len(tokenizer.tokenize(x)) for x in clean_sentences]))\n",
        "all_input = tokenizer(clean_sentences, padding = True, truncation = True, max_length = actual_max)\n",
        "print(\"Actual max length:\", actual_max)\n",
        "\n",
        "#Create loaders\n",
        "train_loader, val_loader, _ = prepare_datasets(cv = True, train_idx = np.arange(len(clean_sentences)))\n",
        "\n",
        "#TUNE\n",
        "start = datetime.now()\n",
        "print(\"Tuning...\", flush = True)\n",
        "study = tune_parameters(50)\n",
        "print(\"Total tuning time: %s\\n\" % (datetime.now() - start), flush = True)\n",
        "\n",
        "best_trial = study.best_trial\n",
        "best_params = best_trial.params\n",
        "print(\"BEST:\", best_trial.value)\n",
        "print(\"Params:\")\n",
        "for key, value in best_params.items():\n",
        "  print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "#TRAIN\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "args = {\"num_hidden_layers\": best_params[\"num_hidden_layers\"],\n",
        "        \"num_attention_heads\": best_params[\"num_attention_heads\"],\n",
        "        \"hidden_dropout_prob\": best_params[\"dropout\"],\n",
        "        \"attention_dropout_prob\": best_params[\"dropout\"],\n",
        "        \"hidden_act\": \"relu\"}\n",
        "\n",
        "model = get_plm(checkpoint, num_class, args).to(device)\n",
        "model.resize_token_embeddings(len(tokenizer))         #Resize vocab for added emojis and reserved tokens\n",
        "optimizer = optim.Adam(model.parameters(), lr = best_params[\"learning_rate\"], weight_decay = best_params[\"weight_decay\"])\n",
        "\n",
        "print(\"=\" * 20, \"MODEL CONFIG\", \"=\" * 20)\n",
        "print(model.config)\n",
        "\n",
        "train_model(epochs = best_params[\"num_epochs\"])\n",
        "\n",
        "torch.save(model, filePath_model) #Save model\n",
        "#uploadFile(filePath_model, filePath_model.split(\"/\")[-1])"
      ],
      "metadata": {
        "id": "ROdUzNMeKAp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MentalBERT"
      ],
      "metadata": {
        "id": "886qrVQDKDCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "print(\"=\" * 50)\n",
        "print(\"=\" * 20, \"MentalBERT\", \"=\" * 20)\n",
        "print(\"=\" * 50)\n",
        "print()\n",
        "\n",
        "#from huggingface_hub import notebook_login\n",
        "#notebook_login()\n",
        "\n",
        "#SETUP\n",
        "checkpoint = \"mental/mental-bert-base-uncased\"\n",
        "filePath_model = \"_MODELS/%s.pt\" % ( checkpoint.replace(\"/\", \"_\"))\n",
        "\n",
        "tokenizer = get_tokenizer(\"wp\", checkpoint)\n",
        "\n",
        "#Create input\n",
        "actual_max = min(MAX_LENGTH, max([len(tokenizer.tokenize(x)) for x in clean_sentences]))\n",
        "all_input = tokenizer(clean_sentences, padding = True, truncation = True, max_length = actual_max)\n",
        "print(\"Actual max length:\", actual_max)\n",
        "\n",
        "#Create loaders\n",
        "train_loader, val_loader, _ = prepare_datasets(cv = True, train_idx = np.arange(len(clean_sentences)))\n",
        "\n",
        "#TUNE\n",
        "start = datetime.now()\n",
        "print(\"Tuning...\", flush = True)\n",
        "study = tune_parameters(50)\n",
        "print(\"Total tuning time: %s\\n\" % (datetime.now() - start), flush = True)\n",
        "\n",
        "best_trial = study.best_trial\n",
        "best_params = best_trial.params\n",
        "print(\"BEST:\", best_trial.value)\n",
        "print(\"Params:\")\n",
        "for key, value in best_params.items():\n",
        "  print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "#TRAIN\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "args = {\"num_hidden_layers\": best_params[\"num_hidden_layers\"],\n",
        "        \"num_attention_heads\": best_params[\"num_attention_heads\"],\n",
        "        \"hidden_dropout_prob\": best_params[\"dropout\"],\n",
        "        \"attention_dropout_prob\": best_params[\"dropout\"],\n",
        "        \"hidden_act\": \"relu\"}\n",
        "\n",
        "model = get_plm(checkpoint, num_class, args).to(device)\n",
        "model.resize_token_embeddings(len(tokenizer))         #Resize vocab for added emojis and reserved tokens\n",
        "optimizer = optim.Adam(model.parameters(), lr = best_params[\"learning_rate\"], weight_decay = best_params[\"weight_decay\"])\n",
        "\n",
        "print(\"=\" * 20, \"MODEL CONFIG\", \"=\" * 20)\n",
        "print(model.config)\n",
        "\n",
        "train_model(epochs = best_params[\"num_epochs\"])\n",
        "torch.save(model, filePath_model) #Save model"
      ],
      "metadata": {
        "id": "LA4zUok3KCjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ClinicalBERT"
      ],
      "metadata": {
        "id": "R1wsbkllkaEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "print(\"=\" * 50)\n",
        "print(\"=\" * 20, \"ClinicalBERT\", \"=\" * 20)\n",
        "print(\"=\" * 50)\n",
        "print()\n",
        "\n",
        "#from huggingface_hub import notebook_login\n",
        "#notebook_login()\n",
        "\n",
        "#SETUP\n",
        "checkpoint = \"medicalai/ClinicalBERT\"\n",
        "filePath_model = \"_MODELS/%s_%s.pt\" % (datasetName, checkpoint.replace(\"/\", \"_\"))\n",
        "\n",
        "tokenizer = get_tokenizer(\"wp\", checkpoint)\n",
        "\n",
        "#Create input\n",
        "actual_max = min(MAX_LENGTH, max([len(tokenizer.tokenize(x)) for x in clean_sentences]))\n",
        "all_input = tokenizer(clean_sentences, padding = True, truncation = True, max_length = actual_max)\n",
        "print(\"Actual max length:\", actual_max)\n",
        "\n",
        "#Create loaders\n",
        "train_loader, val_loader, _ = prepare_datasets(cv = True, train_idx = np.arange(len(clean_sentences)))\n",
        "\n",
        "#TUNE\n",
        "start = datetime.now()\n",
        "print(\"Tuning...\", flush = True)\n",
        "study = tune_parameters()\n",
        "print(\"Total tuning time: %s\\n\" % (datetime.now() - start), flush = True)\n",
        "\n",
        "best_trial = study.best_trial\n",
        "best_params = best_trial.params\n",
        "print(\"BEST:\", best_trial.value)\n",
        "print(\"Params:\")\n",
        "for key, value in best_params.items():\n",
        "  print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "#TRAIN\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "args = {\"num_hidden_layers\": best_params[\"num_hidden_layers\"],\n",
        "        \"num_attention_heads\": best_params[\"num_attention_heads\"],\n",
        "        \"dropout\": best_params[\"dropout\"],\n",
        "        \"attention_dropout\": best_params[\"dropout\"],\n",
        "        \"hidden_act\": \"relu\"}\n",
        "\n",
        "model = get_plm(checkpoint, num_class, args).to(device)\n",
        "model.resize_token_embeddings(len(tokenizer))         #Resize vocab for added emojis and reserved tokens\n",
        "optimizer = optim.Adam(model.parameters(), lr = best_params[\"learning_rate\"], weight_decay = best_params[\"weight_decay\"])\n",
        "\n",
        "print(\"=\" * 20, \"MODEL CONFIG\", \"=\" * 20)\n",
        "print(model.config)\n",
        "\n",
        "train_model(epochs = best_params[\"num_epochs\"])\n",
        "torch.save(model, filePath_model) #Save model"
      ],
      "metadata": {
        "id": "qCo2_1OSkcFM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}