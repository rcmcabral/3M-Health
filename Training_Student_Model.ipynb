{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WqVfcAfP4fR"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUU7Of1XG-eS"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "from tabulate import tabulate\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models, transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "# from tqdm.notebook import tqdm\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRPLfBp-a0dX"
      },
      "outputs": [],
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set Variables"
      ],
      "metadata": {
        "id": "UOjwUlLe-3lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Teacher checkpoints\n",
        "#Select at least 1:\n",
        "#- BERT: \"bert-base-uncased\"\n",
        "#- RoBERTa: \"roberta-base\"\n",
        "#- MentalBERT: \"mental/mental-bert-base-uncased\"\n",
        "#- MMEMOG: \"custom/MM-EMOG-SenticNet\"\n",
        "#- AST: \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
        "teacher_checkpoints = [\"roberta-base\", \"custom/MM-EMOG-SenticNet\", \"MIT/ast-finetuned-audioset-10-10-0.4593\"]\n",
        "\n",
        "#Teacher model paths (path to each saved pytorch teacher models)\n",
        "model_paths = {\n",
        "  \"bert-base-uncased\": \"\",\n",
        "  \"roberta-base\": \"\",\n",
        "  \"mental/mental-bert-base-uncased\": \"\",\n",
        "  \"custom/MM-EMOG-SenticNet\": \"\",\n",
        "  \"MIT/ast-finetuned-audioset-10-10-0.4593\": \"\"\n",
        "}\n",
        "teacher_model_paths = [model_paths[x] for x in teacher_checkpoints]\n",
        "\n",
        "#Student model type\n",
        "student_checkpoint = \"Transformer\"\n",
        "\n",
        "#Session Title. For file saving.\n",
        "sessionTitle = \"Multi-modal Multi-teacher Knowledge Distillation\""
      ],
      "metadata": {
        "id": "BEFLS7QzVE5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load data"
      ],
      "metadata": {
        "id": "TnoZ3EwUjNlp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0ZrSuoxNmdM"
      },
      "outputs": [],
      "source": [
        "#Data Preparation\n",
        "original_train_sentences = []\n",
        "original_val_sentences = []\n",
        "original_test_sentences = []\n",
        "\n",
        "original_train_labels = []\n",
        "original_val_labels = []\n",
        "original_test_labels = []\n",
        "\n",
        "#NOTE: Load audio using librosa\n",
        "# librosa.load(wavPath, sr = 16000) #AST required sample rate\n",
        "original_train_audio = []\n",
        "original_val_audio = []\n",
        "original_test_audio = []\n",
        "\n",
        "assert len(original_train_sentences) == len(original_train_labels) == len(original_train_audio)\n",
        "assert len(original_val_sentences) == len(original_val_labels) == len(original_val_audio)\n",
        "assert len(original_test_sentences) == len(original_test_labels) == len(original_test_audio)\n",
        "\n",
        "train_size = len(original_train_sentences)\n",
        "val_size = len(original_val_sentences)\n",
        "test_size = len(original_test_sentences)\n",
        "\n",
        "train_idx = np.arange(0, train_size)\n",
        "val_idx = np.arange(train_size, train_size + val_size)\n",
        "test_idx = np.arange(train_size + val_size, train_size + val_size + test_size)\n",
        "\n",
        "all_sentences = np.array(original_train_sentences + original_val_sentences + original_test_sentences)\n",
        "all_labels = np.array(original_train_labels + original_val_labels + original_test_labels)\n",
        "all_audio = original_train_audio + original_val_audio + original_test_audio\n",
        "\n",
        "#Label Encoding\n",
        "unique_labels = np.unique(original_train_labels)\n",
        "num_class = len(unique_labels)\n",
        "\n",
        "lEnc = LabelEncoder()\n",
        "lEnc.fit(unique_labels)\n",
        "\n",
        "print(unique_labels)\n",
        "print(lEnc.transform(unique_labels))\n",
        "\n",
        "all_targets = lEnc.transform(all_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3QSsuKsDaz4"
      },
      "source": [
        "#Load Resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKfgCH5aDQuG"
      },
      "outputs": [],
      "source": [
        "#Load list of emoticons\n",
        "#Source: https://c.r74n.com/faces\n",
        "\n",
        "with open(\"TextEmoticonList.txt\", \"r\") as file:\n",
        "  emoticonList = file.read().split(\"\\n\")\n",
        "\n",
        "#Remove emoticons with spaces in-between\n",
        "emoticonList = [emoticon for emoticon in emoticonList if len(emoticon.split(\" \")) == 1]\n",
        "\n",
        "#Remove one character emoticons\n",
        "emoticonList = [emoticon for emoticon in emoticonList if len(emoticon) > 1]\n",
        "\n",
        "print(len(emoticonList))\n",
        "print(emoticonList[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Knh2n_bAZOrF"
      },
      "outputs": [],
      "source": [
        "#Load list of emojis\n",
        "#Source: https://www.airtable.com/universe/exphjm5ifnV0bX4Kb/emojis-database?explore=true\n",
        "\n",
        "emojiList = pd.read_csv(\"Emojis-Grid view.csv\")\n",
        "emojiList = emojiList[emojiList[\"Emoji\"] != \"C\"]\n",
        "emojiList = emojiList[\"Emoji\"].tolist()\n",
        "\n",
        "#Unicode versions\n",
        "emojiList_uni = [emoji.encode('unicode-escape').decode('ASCII') for emoji in emojiList]\n",
        "\n",
        "print(len(emojiList))\n",
        "print(emojiList[:10])\n",
        "print(emojiList_uni[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WVujY7b0MpD"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text"
      ],
      "metadata": {
        "id": "E00tYga8oNC1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbws3oNj7nSP"
      },
      "outputs": [],
      "source": [
        "#FLAGS\n",
        "DEIDENTIFY = True     #Replace urls, emails, and usernames\n",
        "EMOPRESERVE = True    #Identify emojis/emoticons on text and skip text cleaning on them\n",
        "TEXTCLEAN = False     #Minimal cleaning of separating certain conjunctions\n",
        "TOKEN_TYPE = \"wp\"     #wp: word piece (BERT Tokenizer); ws: word split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D4_Mb-y0MpE"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "tokenURL = \"_URL_\"\n",
        "tokenEmail = \"_EMAIL_\"\n",
        "tokenUsername = \"_USER_\"\n",
        "reserveTokens = [tokenURL, tokenEmail, tokenUsername]\n",
        "\n",
        "#CLEANING PROCESS\n",
        "#- Include emojis and emoticons\n",
        "#- Replace url, email, and usernames with tokens\n",
        "#- Remove non-major puncutations and separate them from words with whitespaces\n",
        "#- Lowercase\n",
        "def preprocess_str(string):\n",
        "\n",
        "  #Preclean\n",
        "  if DEIDENTIFY:\n",
        "    string = re.sub(r\"https?://[^\\s]+\", tokenURL, string)              #Links\n",
        "    string = re.sub(r\"[\\w.+-]+@[\\w-]+\\.[\\w.-]+\", tokenEmail, string)   #Email\n",
        "    string = re.sub(r\"@[a-zA-Z0-9_]{2,}\", tokenUsername, string)       #Usernames\n",
        "\n",
        "  #Emoticon/Emoji split\n",
        "  tokens = [string]\n",
        "  if EMOPRESERVE:\n",
        "    allEmo = emoticonList + emojiList + emojiList_uni + reserveTokens\n",
        "    for emoticon in allEmo:\n",
        "      regEx = \"(^|\\s)\" + re.escape(emoticon) + \"(\\s|$)\" if emoticon.isalpha() else re.escape(emoticon)\n",
        "      if emoticon in string:\n",
        "        splits = []\n",
        "        for split in tokens:\n",
        "          splits.append(re.split(r\"(\" + regEx + \")\", split))\n",
        "        tokens = [y.strip() for x in splits for y in x if y != \"\"]\n",
        "\n",
        "  for idx in range(len(tokens)):\n",
        "    if EMOPRESERVE and tokens[idx] in allEmo: #Skip emoticons, emojis\n",
        "      continue\n",
        "\n",
        "    if TEXTCLEAN:\n",
        "      tokens[idx] = re.sub(r\"[^A-Za-z0-9(),!?\\.\\'\\`]\", \" \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\'s\", \" \\'s\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\'ve\", \" \\'ve\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"n\\'t\", \" n\\'t\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\'re\", \" \\'re\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\'d\", \" \\'d\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\'ll\", \" \\'ll\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\",\", \" , \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"!\", \" ! \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\(\", \" ( \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\)\", \" ) \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\?\", \" ? \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\.\", \" . \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\s{2,}\", \" \", tokens[idx])\n",
        "\n",
        "    #Lower case and strip by default\n",
        "    tokens[idx] = tokens[idx].lower().strip()\n",
        "\n",
        "  return \" \".join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjMJlTN60MpF"
      },
      "source": [
        "##Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KsklvB7KNwt"
      },
      "outputs": [],
      "source": [
        "def get_tokenizer(token_type, checkpoint = None):\n",
        "  if token_type.lower() == \"wp\":\n",
        "    if checkpoint in [None, \"bert-base-uncased\", \"custom/MM-EMOG-SenticNet\"]:\n",
        "      from transformers import BertTokenizer\n",
        "      tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    else:\n",
        "      from transformers import AutoTokenizer\n",
        "      tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "    if DEIDENTIFY:\n",
        "      tokenizer.add_tokens(reserveTokens)\n",
        "\n",
        "    if EMOPRESERVE:\n",
        "      #Add spaces to alpha emotions to avoid splitting words that commonly has them (ie \"omo\" in \"tomorrow\")\n",
        "      temp = [\" %s \" % x if x.isalpha() else x for x in emoticonList]\n",
        "      tokenizer.add_tokens(temp + emojiList + emojiList_uni)\n",
        "\n",
        "    return tokenizer\n",
        "  elif token_type.lower() == \"ws\":\n",
        "    return string.split()\n",
        "  else:\n",
        "    raise Exception(\"Unknown value for TOKEN_TYPE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Audio: AST"
      ],
      "metadata": {
        "id": "ncMBQBObtL6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_norm_stats(checkpoint, max_ast_length, sampling_rate):\n",
        "  temp = ASTFeatureExtractor.from_pretrained(checkpoint, max_length = max_ast_length, do_normalize = False)\n",
        "  temp_input = temp(all_audio, sampling_rate = sampling_rate, return_tensors = \"pt\")\n",
        "\n",
        "  return torch.mean(temp_input[\"input_values\"]), torch.std(temp_input[\"input_values\"])"
      ],
      "metadata": {
        "id": "OZ8EImNlKcb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdx6RrUvjbF0"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
        "from transformers import AutoFeatureExtractor, ASTForAudioClassification, ASTFeatureExtractor\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "def get_plm(checkpoint, num_class, args = None):\n",
        "\n",
        "  if checkpoint in [\"bert-base-uncased\", \"roberta-base\", \"mental/mental-bert-base-uncased\"]:\n",
        "    config = AutoConfig.from_pretrained(checkpoint, num_labels = num_class)\n",
        "\n",
        "    if args != None:\n",
        "      config = AutoConfig.from_pretrained(checkpoint, num_labels = num_class, **args)\n",
        "    else:\n",
        "      config = AutoConfig.from_pretrained(checkpoint, num_labels = num_class)\n",
        "\n",
        "    return AutoModelForSequenceClassification.from_pretrained(checkpoint, config = config)\n",
        "  elif checkpoint == \"MIT/ast-finetuned-audioset-10-10-0.4593\":\n",
        "    ast = AST(num_class, args = args)\n",
        "    return ast\n",
        "  # elif checkpoint == \"vgg\":\n",
        "  #   vgg = VGG(num_class)\n",
        "  #   return vgg\n",
        "  elif checkpoint.split(\"/\")[0] == \"custom\":\n",
        "    assert \"pt_weights\" in args\n",
        "    assert \"pt_weights_dim\" in args\n",
        "\n",
        "    return MLP(num_class = num_class, **args)\n",
        "  else:\n",
        "    raise Exception(\"Unknown checkpoint\")"
      ],
      "metadata": {
        "id": "95axGmFvtO-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_student_model(st_checkpoint, params = None):\n",
        "  # if st_checkpoint == \"MLP\":\n",
        "  #   return MLP(pt_weights, pt_weights.shape[-1], num_class, params[\"num_layers\"], params[\"num_hidden\"], params[\"dropout\"], max_length = actual_max)\n",
        "\n",
        "  if st_checkpoint == \"Transformer\":\n",
        "    if params != None:\n",
        "      assert \"num_hidden_layers\" in params\n",
        "      assert \"num_attention_heads\" in params\n",
        "      assert \"hidden_dropout_prob\" in params\n",
        "      assert \"hidden_act\" in params\n",
        "\n",
        "      config = AutoConfig.from_pretrained(\"bert-base-uncased\", num_labels = num_class, **params)\n",
        "    else:\n",
        "      config = AutoConfig.from_pretrained(\"bert-base-uncased\", num_labels = num_class)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", config = config)\n",
        "    model.resize_token_embeddings(len(student_tokenizer))\n",
        "    return model\n",
        "\n",
        "  raise Exception(\"Unknown student checkpoint.\")"
      ],
      "metadata": {
        "id": "I_VJFodcvVvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AST"
      ],
      "metadata": {
        "id": "g4jlI5qxwXha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Wraps AST into model class that returns dict\n",
        "class AST(torch.nn.Module):\n",
        "  def __init__(self, num_class, args = None):\n",
        "    super(AST, self).__init__()\n",
        "\n",
        "    #Load config\n",
        "    config = AutoConfig.from_pretrained(checkpoint, num_labels = num_class)\n",
        "\n",
        "    if args != None:\n",
        "      config = AutoConfig.from_pretrained(checkpoint, num_labels = num_class, **args)\n",
        "    else:\n",
        "      config = AutoConfig.from_pretrained(checkpoint, num_labels = num_class)\n",
        "\n",
        "    #Load pretrained model\n",
        "    self.ast = ASTForAudioClassification.from_pretrained(checkpoint, ignore_mismatched_sizes = True, config = config)\n",
        "\n",
        "    #Change last classification layer output to num classes\n",
        "    # self.ast.classifier.dense = nn.Linear(in_features = 768, out_features = num_class, bias = True)\n",
        "\n",
        "  def forward(self, input_values, labels = None):\n",
        "\n",
        "    x = self.ast(input_values = input_values, labels = labels)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Oa2qim3AwN5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MLP"
      ],
      "metadata": {
        "id": "40CnqEn-M7A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, pt_weights, pt_weights_dim, num_class, num_layers, hidden_dim, dropout, max_length):\n",
        "    super(MLP, self).__init__()\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    #Load Embeddings\n",
        "    self.embeddings = nn.Embedding.from_pretrained(pt_weights)\n",
        "\n",
        "    #MLP\n",
        "    if self.num_layers >= 2:\n",
        "      self.l1 = nn.Linear(max_length * pt_weights_dim, hidden_dim)\n",
        "      self.r1 = nn.ReLU()\n",
        "      self.d1 = nn.Dropout(dropout)\n",
        "\n",
        "      moduleList = []\n",
        "      for _ in range(num_layers - 2):\n",
        "        moduleList.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "        moduleList.append(nn.ReLU())\n",
        "        moduleList.append(nn.Dropout(dropout))\n",
        "\n",
        "      self.mod_list = nn.ModuleList(moduleList)\n",
        "      self.lf = nn.Linear(hidden_dim, num_class)\n",
        "      self.rf = nn.ReLU()\n",
        "      self.df = nn.Dropout(dropout)\n",
        "    else:\n",
        "      self.l1 = nn.Linear(max_length * pt_weights_dim, num_class)\n",
        "      self.r1 = nn.ReLU()\n",
        "      self.d1 = nn.Dropout(dropout)\n",
        "    # self.softmax = nn.Softmax()\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "\n",
        "    #Generate embeddings\n",
        "    features = self.embeddings(input_ids)      # embedded = [batch size, sent_len, emb dim]\n",
        "\n",
        "    # features = torch.cat((bert_features, mmemog_features), axis = -1)\n",
        "    x = features.view(features.shape[0], -1)  #Flatten\n",
        "\n",
        "    if self.num_layers >= 2:\n",
        "      x = self.l1(x)\n",
        "      x = self.r1(x)\n",
        "      x = self.d1(x)\n",
        "\n",
        "      for i in range(self.num_layers - 2):\n",
        "        x = self.mod_list[i](x)\n",
        "\n",
        "      x = self.lf(x)\n",
        "      x = self.rf(x)\n",
        "      x = self.df(x)\n",
        "    else:\n",
        "      x = self.l1(x)\n",
        "      x = self.r1(x)\n",
        "      x = self.d1(x)\n",
        "    # output = self.softmax(x)\n",
        "    output = {\"logits\": x}\n",
        "    return output"
      ],
      "metadata": {
        "id": "pTQvvKseM8YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEE4JxeUthCb"
      },
      "source": [
        "# Training Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIxII4QoticA"
      },
      "source": [
        "## Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MTKDDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, inputs, labels, ids):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "        self.ids = ids\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if type(self.inputs) == dict:\n",
        "          item = {key: val[idx] for key, val in self.inputs.items()}\n",
        "        else:\n",
        "          item = {\"input\": self.inputs[idx]}\n",
        "\n",
        "        item[\"ids\"] = self.ids[idx]\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "nEOGvcLqOeC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CURL_CA_BUNDLE'] = ''"
      ],
      "metadata": {
        "id": "FANET2fvsVBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gather all inputs for all models\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "student_tokenizer = get_tokenizer(\"wp\", \"bert-base-uncased\")\n",
        "\n",
        "def build_inputs(sentences, audio, max_lengths = {}, return_max_len = False):\n",
        "  inputs = {}\n",
        "  clean_sentences = [preprocess_str(x) for x in sentences]\n",
        "\n",
        "  #Teacher inputs\n",
        "  for tcheck in teacher_checkpoints:\n",
        "    print(\"Building inputs for\", tcheck)\n",
        "    if tcheck not in [\"bert-base-uncased\", \"roberta-base\", \"mental/mental-bert-base-uncased\", \"custom/MM-EMOG-SenticNet\", \"MIT/ast-finetuned-audioset-10-10-0.4593\"]:\n",
        "      raise Exception(\"Unhandled checkpoint\")\n",
        "\n",
        "    if tcheck == \"MIT/ast-finetuned-audioset-10-10-0.4593\":\n",
        "      MAX_AST = 512\n",
        "      mean, std = get_norm_stats(tcheck, MAX_AST, SAMPLING_RATE)\n",
        "\n",
        "      #Extract features\n",
        "      feature_extractor = ASTFeatureExtractor.from_pretrained(tcheck, max_length = MAX_AST, do_normalize = True, mean = float(mean), std = float(std))\n",
        "      input = feature_extractor(audio, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\")\n",
        "      print(\"AST -> Check mean 0, std 0.5:\", torch.mean(input[\"input_values\"]), torch.std(input[\"input_values\"]))\n",
        "\n",
        "    else: #Standard PLM inputs\n",
        "      targs = {}\n",
        "      ttokenizer = get_tokenizer(\"wp\", tcheck)\n",
        "      if tcheck == \"custom/MM-EMOG-SenticNet\":\n",
        "        targs[\"return_token_type_ids\"] = False\n",
        "        targs[\"return_attention_mask\"] = False\n",
        "\n",
        "      if tcheck not in max_lengths.keys():\n",
        "        actual_max = min(MAX_LENGTH, max([len(ttokenizer.tokenize(x)) for x in clean_sentences]))\n",
        "        max_lengths[tcheck] = actual_max\n",
        "      else:\n",
        "        actual_max = max_lengths[tcheck]\n",
        "      input = ttokenizer(clean_sentences, padding = \"max_length\", truncation = True, max_length = actual_max, **targs)\n",
        "\n",
        "    for k, v in input.items():\n",
        "      dtype = torch.FloatTensor if tcheck == \"MIT/ast-finetuned-audioset-10-10-0.4593\" else torch.LongTensor\n",
        "      inputs[tcheck + \"|\" + k] = dtype(v)\n",
        "\n",
        "  #Build student inputs\n",
        "  #BERT model with BERT inputs\n",
        "  if student_checkpoint == \"Transformer\":\n",
        "    if \"student\" not in max_lengths.keys():\n",
        "      actual_max = min(MAX_LENGTH, max([len(student_tokenizer.tokenize(x)) for x in clean_sentences]))\n",
        "      max_lengths[\"student\"] = actual_max\n",
        "    else:\n",
        "      actual_max = max_lengths[\"student\"]\n",
        "    student_input = student_tokenizer(clean_sentences, padding = \"max_length\", truncation = True, max_length = actual_max)\n",
        "\n",
        "  else:\n",
        "    raise Exception(\"Unknown student checkpoint.\")\n",
        "\n",
        "  for k, v in student_input.items():\n",
        "    inputs[\"student|\" + k] = torch.LongTensor(v)\n",
        "\n",
        "  if return_max_len:\n",
        "    return inputs, max_lengths\n",
        "  return inputs"
      ],
      "metadata": {
        "id": "mfFKC5-cs-WJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_datasets(sentences, targets, audios, train_percent = 0.9, cv = False, train_idx = [], test_idx = [], show_result = False):\n",
        "\n",
        "  \"\"\"Generate splits\"\"\"\n",
        "  np.random.seed(123)\n",
        "\n",
        "  #Cross validation / Force split train set to train & val\n",
        "  if cv == True:\n",
        "    assert len(train_idx) > 0\n",
        "    #assert len(test_idx) > 0 #If test_idx = none -> split only train/val, test with all data\n",
        "\n",
        "    idx_train = np.random.choice(train_idx, int(len(train_idx) * train_percent), replace = False)\n",
        "    idx_val = [x for x in train_idx if x not in idx_train]\n",
        "    idx_test = test_idx\n",
        "\n",
        "  #Train and test only\n",
        "  elif (val_size == 0) and (test_size != 0):\n",
        "    idx_train = np.random.choice(np.arange(train_size), int(train_size * train_percent), replace = False)\n",
        "    idx_val = [x for x in np.arange(train_size) if x not in idx_train]\n",
        "    idx_test = np.arange(train_size, len(all_sentences))\n",
        "\n",
        "  #Train, val, and test\n",
        "  elif val_size != 0:\n",
        "    idx_train = np.arange(0, train_size)\n",
        "    idx_val = np.arange(train_size, train_size + val_size)\n",
        "    idx_test = np.arange(train_size + val_size, len(all_sentences))\n",
        "  else:\n",
        "    raise Exception(\"Unknown split.\")\n",
        "\n",
        "  print(\"Data Loader split:\")\n",
        "  print(\"  - Train:\", len(idx_train))\n",
        "  print(\"  - Val:\", len(idx_val))\n",
        "  print(\"  - Test:\", len(idx_test))\n",
        "\n",
        "  \"\"\"Generate inputs\"\"\"\n",
        "  print(\"Training inputs\")\n",
        "  train_inputs, max_lengths = build_inputs(sentences[idx_train], [audios[i] for i in idx_train], return_max_len = True) #Extract max_lengths to implement on val and test sets\n",
        "  print(\"Validation inputs\")\n",
        "  val_inputs = build_inputs(sentences[idx_val], [audios[i] for i in idx_val], max_lengths = max_lengths)\n",
        "  print(\"Test inputs\")\n",
        "  test_inputs = build_inputs(sentences[idx_test], [audios[i] for i in idx_test], max_lengths = max_lengths) if len(idx_test) > 0 else []\n",
        "\n",
        "  \"\"\"Prepare loaders\"\"\"\n",
        "\n",
        "  train_targets = torch.LongTensor(targets[idx_train])\n",
        "  val_targets = torch.LongTensor(targets[idx_val])\n",
        "  test_targets = torch.LongTensor(targets[idx_test])\n",
        "\n",
        "  #Shuffle is turned off to ensure multiple loaders loading the same samples in the same order\n",
        "  train_loader = torch.utils.data.DataLoader(MTKDDataset(train_inputs, train_targets, idx_train), shuffle=True, batch_size = BATCH_SIZE)\n",
        "  val_loader = torch.utils.data.DataLoader(MTKDDataset(val_inputs, val_targets, idx_val), shuffle = True, batch_size = BATCH_SIZE)\n",
        "  test_loader = torch.utils.data.DataLoader(MTKDDataset(test_inputs, test_targets, idx_test), shuffle = False, batch_size = BATCH_SIZE)\n",
        "\n",
        "  if len(test_targets) == 0:\n",
        "    return train_loader, val_loader\n",
        "  return train_loader, val_loader, test_loader\n"
      ],
      "metadata": {
        "id": "e4dfzKHuKRIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train"
      ],
      "metadata": {
        "id": "fujgnhJXcOYV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QC7u3Jn2uIu4"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_ce = torch.nn.CrossEntropyLoss()\n",
        "criterion_kl = torch.nn.KLDivLoss()\n",
        "weight_ce = 1.0\n",
        "weight_kl = 1.0\n",
        "\n",
        "#Training student with multi-teacher\n",
        "def train_student(st_model, optimizer, epoch, show_result = True):\n",
        "  loss_batch_train = []\n",
        "  acc_batch_train = []\n",
        "  f1_batch_train = []\n",
        "  loss_batch_val = []\n",
        "  acc_batch_val = []\n",
        "  f1_batch_val = []\n",
        "\n",
        "  t = time.time()\n",
        "\n",
        "  #TRAINING\n",
        "  st_model.train()\n",
        "  for i, batch in enumerate(train_loader):\n",
        "\n",
        "    targets = batch[\"labels\"].to(device)\n",
        "    current_batch_size = len(batch[\"labels\"])\n",
        "\n",
        "    #Compute teacher outputs\n",
        "    tm_scores = []\n",
        "    # hint_maps = []\n",
        "    for check in teacher_checkpoints:\n",
        "      #Get teacher inputs\n",
        "      input_args = {k.split(check + \"|\")[-1]: v.to(device) for k, v in batch.items() if k.startswith(check)}\n",
        "      tm = teacher_models[check]\n",
        "      tm.eval()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        tm_output = tm(**input_args)[\"logits\"]\n",
        "\n",
        "        tm_output = F.softmax(tm_output, dim = -1)\n",
        "        tm_scores.append(tm_output)\n",
        "\n",
        "    tm_scores_Tensor = torch.stack(tm_scores, dim = 1)\n",
        "    mean_scores = torch.mean(tm_scores_Tensor, dim = 1)\n",
        "\n",
        "    #Compute student outputs\n",
        "    input_args = {k.split(\"student|\")[-1]: v.to(device) for k, v in batch.items() if k.startswith(\"student\")}\n",
        "    st_output = st_model(**input_args)[\"logits\"]\n",
        "\n",
        "    #Compute gradients - MFH computation\n",
        "    kd_loss = weight_ce * criterion_ce(st_output, targets) + weight_kl * criterion_kl(torch.log_softmax(st_output, dim=1), mean_scores)\n",
        "    loss = kd_loss\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward(retain_graph = True)\n",
        "    optimizer.step()\n",
        "\n",
        "    st_output = st_output.float()\n",
        "    loss = loss.float()\n",
        "\n",
        "    train_acc = accuracy(st_output.data, targets.data)[0]\n",
        "    train_f1 = f1_score(targets.cpu(), np.argmax(st_output.cpu().detach().numpy(), axis = -1), average = \"weighted\")\n",
        "\n",
        "    loss_batch_train.append(loss.item())\n",
        "    acc_batch_train.append(train_acc.cpu())\n",
        "    f1_batch_train.append(train_f1)\n",
        "\n",
        "  #VALIDATION\n",
        "  st_model.eval()\n",
        "  for i, batch in enumerate(val_loader):\n",
        "\n",
        "    targets = batch[\"labels\"].to(device)\n",
        "    current_batch_size = len(batch[\"labels\"])\n",
        "\n",
        "    #Compute teacher outputs\n",
        "    tm_scores = []\n",
        "    # hint_maps = []\n",
        "    for check in teacher_checkpoints:\n",
        "      #Get teacher inputs\n",
        "      input_args = {k.split(check + \"|\")[-1]: v.to(device) for k, v in batch.items() if k.startswith(check)}\n",
        "      tm = teacher_models[check]\n",
        "      tm.eval()\n",
        "      with torch.no_grad():\n",
        "        tm_output = tm(**input_args)[\"logits\"]\n",
        "\n",
        "        tm_output = F.softmax(tm_output, dim = -1)\n",
        "        tm_scores.append(tm_output)\n",
        "\n",
        "    tm_scores_Tensor = torch.stack(tm_scores, dim = 1)\n",
        "    mean_scores = torch.mean(tm_scores_Tensor, dim = 1)\n",
        "\n",
        "    #Compute student outputs\n",
        "    input_args = {k.split(\"student|\")[-1]: v.to(device) for k, v in batch.items() if k.startswith(\"student\")}\n",
        "    with torch.no_grad():\n",
        "      st_output = st_model(**input_args)[\"logits\"]\n",
        "\n",
        "      #Compute gradients - MFH computation\n",
        "      kd_loss = weight_ce * criterion_ce(st_output, targets) + weight_kl * criterion_kl(torch.log_softmax(st_output, dim=1), mean_scores)\n",
        "      loss = kd_loss\n",
        "\n",
        "      val_acc = accuracy(st_output.data, targets.data)[0]\n",
        "      val_f1 = f1_score(targets.cpu(), np.argmax(st_output.cpu().detach().numpy(), axis = -1), average = \"weighted\")\n",
        "\n",
        "      loss_batch_val.append(loss.item())\n",
        "      acc_batch_val.append(val_acc.cpu())\n",
        "      f1_batch_val.append(val_f1)\n",
        "\n",
        "  if show_result:\n",
        "    print(  'Epoch: {:04d}'.format(epoch+1),\n",
        "            'loss_train: {:.4f}'.format(np.mean(loss_batch_train)),\n",
        "            'acc_train: {:.4f}'.format(np.mean(acc_batch_train)),\n",
        "            'f1w_train: {:.4f}'.format(np.mean(f1_batch_train)),\n",
        "            'loss_val: {:.4f}'.format(np.mean(loss_batch_val)),\n",
        "            'acc_val: {:.4f}'.format(np.mean(acc_batch_val)),\n",
        "            'f1w_val: {:.4f}'.format(np.mean(f1_batch_val)),\n",
        "            'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "  #return losses for early stopping\n",
        "  return np.mean(loss_batch_train), np.mean(loss_batch_val), np.mean(acc_batch_train), np.mean(acc_batch_val), np.mean(f1_batch_train), np.mean(f1_batch_val)"
      ],
      "metadata": {
        "id": "ralrEZGlUJV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test"
      ],
      "metadata": {
        "id": "-nJyn5NncMK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model():\n",
        "    model.eval()\n",
        "\n",
        "    acc_batch_val = []\n",
        "    output = []\n",
        "    gold = []\n",
        "    for x_batch, y_batch in test_loader:\n",
        "      out = model(x_batch)\n",
        "      output.extend(out.cpu().detach().numpy() )\n",
        "      gold.extend(y_batch.cpu().numpy())\n",
        "\n",
        "    return np.array(gold), np.array(output)"
      ],
      "metadata": {
        "id": "0V4yIILBysQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQwlWq6dyYJm"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ],
      "metadata": {
        "id": "YUivL4HWT4qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjnwvyIDt2Sb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, hamming_loss, roc_auc_score\n",
        "from scipy.special import softmax\n",
        "\n",
        "def evaluate_output(outputs, targets, targetLabels, title = \"\", show_results = True, showClassMatrix = False, multiLabel = False, return_results = False):\n",
        "\n",
        "    if multiLabel:\n",
        "      preds = np.array(outputs) >= 0.5\n",
        "    else:\n",
        "      preds = np.argmax(outputs, axis = 1)\n",
        "      probs = outputs if np.sum(outputs[0]) == 1 else softmax(outputs, axis = -1) #Check if probabilities\n",
        "\n",
        "    accuracy = accuracy_score(targets, preds)\n",
        "    f1_score_micro = f1_score(targets, preds, average='micro')\n",
        "    f1_score_macro = f1_score(targets, preds, average='macro')\n",
        "    f1_score_weighted = f1_score(targets, preds, average=\"weighted\")\n",
        "    if len(targetLabels) > 2:\n",
        "      roc_scores = roc_auc_score(targets, probs, average = None, multi_class = \"ovr\")\n",
        "      roc_type = \"Average\"\n",
        "    else:\n",
        "      roc_scores = roc_auc_score(targets, probs[:,1])\n",
        "      roc_type = \"Positive Class (%s)\" % targetLabels[1]\n",
        "\n",
        "\n",
        "    if show_results:\n",
        "      print()\n",
        "      print(\"=\" * 50)\n",
        "      print(title)\n",
        "      print(\"=\" * 50)\n",
        "      print(\"Accuracy Score: %.4f\" % (accuracy))\n",
        "      print(\"F1 Score (Micro): %.4f\" % (f1_score_micro))\n",
        "      print(\"F1 Score (Macro): %.4f\" % (f1_score_macro))\n",
        "      print(\"F1 Score (Weighted): %.4f\" % (f1_score_weighted))\n",
        "      print(\"ROC AUC (%s): %.4f\" % (roc_type, np.mean(roc_scores)))\n",
        "      print()\n",
        "\n",
        "      if multiLabel:\n",
        "        ham_loss = hamming_loss(targets, preds)\n",
        "        print(\"Hamming Loss: %.4f\" % (ham_loss))\n",
        "\n",
        "      print(targetLabels)\n",
        "      print(classification_report(targets, preds, target_names = targetLabels, digits = 4))\n",
        "      if len(targetLabels) > 2:\n",
        "        print(\"\\nROCAUC\")\n",
        "        print(tabulate([roc_scores], headers = targetLabels, floatfmt=\".4f\"))\n",
        "\n",
        "    if showClassMatrix:\n",
        "      if multiLabel:\n",
        "        cmArray = multilabel_confusion_matrix(targets, preds)\n",
        "        for i in range(len(targetLabels)):\n",
        "          plotConfusionMatrix(cmArray[i], [0,1], targetLabels[i])\n",
        "      else:\n",
        "        cmArray = confusion_matrix(targets, preds)\n",
        "        plotConfusionMatrix(cmArray, np.unique(targetLabels), title)\n",
        "\n",
        "    if return_results:\n",
        "      results = {\"Accuracy\": accuracy,\n",
        "              \"F1_Micro\": f1_score_micro,\n",
        "              \"F1_Macro\": f1_score_macro,\n",
        "              \"F1_Weighted\": f1_score_weighted,\n",
        "              \"Class Precision\": precision_score(targets, preds, average = None),\n",
        "              \"Class Recall\": recall_score(targets, preds, average = None),\n",
        "              \"Class F1\": f1_score(targets, preds, average = None),\n",
        "      }\n",
        "\n",
        "      if len(targetLabels) > 2:\n",
        "        results[\"Class ROCAUC\"] = roc_scores\n",
        "      else:\n",
        "        results[\"ROC_Class1:\"] = roc_scores\n",
        "\n",
        "      return results\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_predictions(ids, targets, outputs, extra_title = \"\"):\n",
        "  saveDF = pd.DataFrame({\"Texts\": all_sentences[ids],\n",
        "                        \"Labels\": lEnc.inverse_transform(targets),\n",
        "                        \"Preds\": lEnc.inverse_transform(np.argmax(outputs, axis = -1))},\n",
        "                        index = ids)\n",
        "  for i, c in enumerate(lEnc.classes_):\n",
        "    saveDF[c] = outputs[:, i]\n",
        "\n",
        "  saveDF = saveDF.sort_index()\n",
        "  saveDF.to_csv(\"%s_%s.csv\" % (sessionTitle, extra_title), index = False)"
      ],
      "metadata": {
        "id": "XlReK-mrPH5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plotConfusionMatrix(cmArray, labels, title, savePath = \"\"):\n",
        "\n",
        "  df_cm = pd.DataFrame(cmArray,\n",
        "                      index = labels,\n",
        "                      columns = labels)\n",
        "\n",
        "  plt.figure(figsize = (5,4))\n",
        "  plt.title(title)\n",
        "  sn.heatmap(df_cm, annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "\n",
        "  if savePath != \"\":\n",
        "    # plt.tight_layout()\n",
        "    plt.savefig(\"%s_results.png\" % savePath, bbox_inches = \"tight\")\n"
      ],
      "metadata": {
        "id": "KvnN-iEbvz6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc26jk8Cnwei"
      },
      "source": [
        "#Tuning Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbRnlCxZnyE5"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "def objective(trial):\n",
        "\n",
        "  tune_dropout = trial.suggest_categorical(\"dropout\", [0.01, 0.05, 0.1, 0.5])\n",
        "\n",
        "  if student_checkpoint == \"Transformer\":\n",
        "    tune_lr = trial.suggest_categorical(\"learning_rate\", [1e-04, 1e-05, 2e-05, 3e-05, 4e-05, 5e-05])\n",
        "    tune_decay = trial.suggest_categorical(\"weight_decay\", [0, 0.01, 0.1])\n",
        "    tune_layers = trial.suggest_int(\"num_hidden_layers\", 2, 12, 2)\n",
        "    tune_heads = trial.suggest_categorical(\"num_attention_heads\", [ 2,  3,  4,  6,  8, 12]) #choose num heads % 768\n",
        "    tune_act = trial.suggest_categorical(\"hidden_act\", [\"relu\", \"gelu\"])\n",
        "\n",
        "    tune_epochs = trial.suggest_int(\"num_epochs\", 3, 5)\n",
        "    tune_stop = None\n",
        "\n",
        "    args = {\n",
        "            \"num_hidden_layers\": tune_layers,\n",
        "            \"num_attention_heads\": tune_heads,\n",
        "            \"hidden_dropout_prob\": tune_dropout,\n",
        "            \"hidden_act\": tune_act\n",
        "            }\n",
        "    tune_model = get_student_model(student_checkpoint, args).to(device)\n",
        "    tune_model.resize_token_embeddings(len(student_tokenizer))         #Resize vocab for added emojis and reserved tokens\n",
        "    optimizer = optim.Adam(tune_model.parameters(), lr = tune_lr, weight_decay = tune_decay)\n",
        "\n",
        "  else:\n",
        "    raise Exception(\"Unknown student checkpoint.\")\n",
        "\n",
        "  #Training\n",
        "  train_loss = []\n",
        "  val_loss = []\n",
        "\n",
        "  for epoch in range(tune_epochs):\n",
        "    tLoss, vLoss, tAcc, vAcc, tF1, f1_val = train_student(tune_model, optimizer, epoch, show_result = False)\n",
        "    train_loss.append(tLoss)\n",
        "    val_loss.append(vLoss)\n",
        "\n",
        "    if tune_stop != None and epoch > tune_stop and np.min(val_loss[-tune_stop:]) > np.min(val_loss[:-tune_stop]) :\n",
        "      # if show_result:\n",
        "      #     print(\"Early Stopping at epoch %d\" % (epoch + 1))\n",
        "      break\n",
        "\n",
        "    #Record metric\n",
        "    trial.report(f1_val, epoch)\n",
        "\n",
        "    # Handle pruning based on the intermediate value.\n",
        "    if trial.should_prune():\n",
        "        raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "  return f1_val\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_parameters(n_trials = 20):\n",
        "  study = optuna.create_study(direction = \"maximize\")\n",
        "\n",
        "  if student_checkpoint == \"Transformer\":\n",
        "    study.enqueue_trial({\"dropout\": 0.5,\n",
        "                        \"learning_rate\": 1e-04,\n",
        "                        \"num_hidden_layers\": 12,\n",
        "                        \"num_attention_heads\": 12,\n",
        "                        \"weight_decay\": 0,\n",
        "                        \"num_epochs\": 3,\n",
        "                        \"hidden_act\": \"relu\"})\n",
        "  else:\n",
        "    raise Exception(\"Unknown student checkpoint.\")\n",
        "  study.optimize(objective, n_trials = n_trials)\n",
        "\n",
        "  return study"
      ],
      "metadata": {
        "id": "GU8z6cRl11mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Knowledge Distillation"
      ],
      "metadata": {
        "id": "wc1S6VgGEseL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Initialize Teachers"
      ],
      "metadata": {
        "id": "fJZ1QaksyfPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "#Load teacher models\n",
        "teacher_models = {}\n",
        "print(\"Teacher models:\")\n",
        "for check, path in zip(teacher_checkpoints, teacher_model_paths):\n",
        "  if path.split(\".\")[-1] == \"pt\": #standard model\n",
        "    tm = torch.load(path)\n",
        "  else: #pretrained model\n",
        "    tm = AutoModel.from_pretrained(path)\n",
        "  print(\"->\", check)\n",
        "  tm.to(device)\n",
        "  tm.eval()\n",
        "  teacher_models[check] = tm"
      ],
      "metadata": {
        "id": "7JJwVhgvGaxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Setup"
      ],
      "metadata": {
        "id": "oC1ZblTSOgZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"_OUTPUT\", exist_ok = True)"
      ],
      "metadata": {
        "id": "lj1PU5SBXfpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "MAX_LENGTH = 256 #Adjusts programmatically if shorter\n",
        "\n",
        "NUM_RUNS = 10\n",
        "show_result = True"
      ],
      "metadata": {
        "id": "85z5GBV-PZ17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyperparameter Selection"
      ],
      "metadata": {
        "id": "Uy7lpatMyQRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load tuning data\n",
        "#CV = True and train_idx = arange all data to force 90/10 split for tuning\n",
        "train_loader, val_loader = prepare_datasets(all_sentences, all_targets, all_audio, cv = True, train_idx = np.arange(len(all_sentences)))"
      ],
      "metadata": {
        "id": "0knaF7CLn62T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TUNE\n",
        "start = datetime.now()\n",
        "print(\"Tuning...\", flush = True)\n",
        "study = tune_parameters(n_trials = 50)\n",
        "print(\"Total tuning time: %s\\n\" % (datetime.now() - start), flush = True)\n",
        "\n",
        "best_trial = study.best_trial\n",
        "best_params = best_trial.params\n",
        "\n",
        "print(\"BEST:\", best_trial.value)\n",
        "print(\"Params:\")\n",
        "for key, value in best_params.items():\n",
        "  print(\"    {}: {}\".format(key, value))"
      ],
      "metadata": {
        "id": "oH9efEeWyRw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run Training\n"
      ],
      "metadata": {
        "id": "8wU-eXppOYfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train & Evaluate Student"
      ],
      "metadata": {
        "id": "inFd1K2cykxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loaders\n",
        "train_loader, val_loader, test_loader = prepare_datasets(all_sentences, all_targets, all_audio)"
      ],
      "metadata": {
        "id": "o1EPv1qnuKwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_run = {\"F1_Weighted\": 0}\n",
        "all_runs = {}\n",
        "for run in range(NUM_RUNS):\n",
        "  print(\"=\" * 50)\n",
        "  print(\"RUN %d\" % (run + 1))\n",
        "  print(\"=\" * 50)\n",
        "  start = datetime.now()\n",
        "\n",
        "  if student_checkpoint == \"Transformer\":\n",
        "    args = {\n",
        "            \"num_hidden_layers\": best_params[\"num_hidden_layers\"],\n",
        "            \"num_attention_heads\": best_params[\"num_attention_heads\"],\n",
        "            \"hidden_dropout_prob\": best_params[\"dropout\"],\n",
        "            \"hidden_act\": best_params[\"hidden_act\"]\n",
        "            }\n",
        "    student_model = get_student_model(student_checkpoint, params = args).to(device)\n",
        "\n",
        "  optimizer = optim.Adam(student_model.parameters(), lr=best_params[\"learning_rate\"], weight_decay=best_params[\"weight_decay\"] if \"weight_decay\" in best_params else 0)\n",
        "  print(student_model.config)\n",
        "\n",
        "  #Training\n",
        "  train_loss = []\n",
        "  val_loss = []\n",
        "\n",
        "  for epoch in range(best_params[\"num_epochs\"] if \"num_epochs\" in best_params else NUM_EPOCHS):\n",
        "    tLoss, vLoss, tAcc, vAcc, tF1, vF1  = train_student(student_model, optimizer, epoch)\n",
        "    train_loss.append(tLoss)\n",
        "    val_loss.append(vLoss)\n",
        "\n",
        "  # plt.plot(train_loss, label = \"train\")\n",
        "  # plt.plot(val_loss, label = \"val\")\n",
        "  # plt.legend()\n",
        "  # plt.show()\n",
        "\n",
        "  #Testing - student only\n",
        "  st_outputs = []\n",
        "  st_targets = []\n",
        "  st_ids = []\n",
        "  for batch in test_loader:\n",
        "    input_args = {k.split(\"student|\")[-1]: v.to(device) for k, v in batch.items() if k.startswith(\"student\")}\n",
        "    with torch.no_grad():\n",
        "      st_output = student_model(**input_args)[\"logits\"]\n",
        "      st_outputs.extend(st_output.cpu().detach().numpy())\n",
        "      st_targets.extend(batch[\"labels\"].cpu().detach().numpy())\n",
        "      st_ids.extend(batch[\"ids\"])\n",
        "\n",
        "  st_ids = torch.stack(st_ids).numpy()\n",
        "  st_outputs = np.array(st_outputs)\n",
        "\n",
        "  #Evaluate\n",
        "  run_results = evaluate_output(st_outputs, st_targets, lEnc.classes_, show_results = False, return_results = True)\n",
        "  for k, v in run_results.items():\n",
        "    if k in all_runs:\n",
        "      all_runs[k].append(v)\n",
        "    else:\n",
        "      all_runs[k] = [v]\n",
        "\n",
        "  print(\"-> RUN %s total time: %s\" % (run, datetime.now() - start))\n",
        "  for k, v in run_results.items():\n",
        "    print(\"-> %s: %s\" % (k, v))\n",
        "\n",
        "  #Save if best run\n",
        "  if run_results[\"F1_Weighted\"] > best_run[\"F1_Weighted\"]:\n",
        "    best_run = run_results\n",
        "    best_run[\"targets\"] = st_targets\n",
        "    best_run[\"outputs\"] = st_outputs\n",
        "    save_predictions(st_ids, st_targets, st_outputs, extra_title = \"_bestOf%d\" % NUM_RUNS)"
      ],
      "metadata": {
        "id": "nhfOcr6iCYwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Best Run\n",
        "title = sessionTitle + \"_best\"\n",
        "for check in teacher_checkpoints:\n",
        "  title += \"\\n->\" + check\n",
        "evaluate_output(best_run[\"outputs\"], best_run[\"targets\"], lEnc.classes_.astype(str), title, showClassMatrix = True)"
      ],
      "metadata": {
        "id": "fxDbFvBMoMXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate average runs\n",
        "out_text = \"\"\n",
        "tab_data = {\"Classes\": lEnc.classes_}\n",
        "for k, v in all_runs.items():\n",
        "  if k.startswith(\"Class\"):\n",
        "    tab_data[k] = np.mean(v, axis = 0)\n",
        "    tab_data[k + \"(±)\"] = np.std(v, axis = 0)\n",
        "  else:\n",
        "    out_text += \"%s: %.4f (± %.4f)\\n\" % (k, np.mean(v), np.std(v))\n",
        "\n",
        "print(out_text)\n",
        "print(tabulate(tab_data, headers = tab_data.keys(), floatfmt=\".4f\"))"
      ],
      "metadata": {
        "id": "BMs_T-GsnUmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save all results\n",
        "pd.DataFrame(all_runs).to_csv(\"_OUTPUT/%s_results.csv\" % (sessionTitle))"
      ],
      "metadata": {
        "id": "roAnELeT8vmP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}